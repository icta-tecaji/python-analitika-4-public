{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d83707a2",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d2423",
   "metadata": {},
   "source": [
    "# Taxi game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd72f95",
   "metadata": {},
   "source": [
    "[Taxi_game documentation](https://gymnasium.farama.org/environments/toy_text/taxi/#taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b456d0f",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/taxi_game.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5888353d",
   "metadata": {},
   "source": [
    "## State space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86719fde",
   "metadata": {},
   "source": [
    "## Action space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29d1ba",
   "metadata": {},
   "source": [
    "## Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4728c13a",
   "metadata": {},
   "source": [
    "## Implementacija v Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea777cb",
   "metadata": {},
   "source": [
    "`$ pip install gymnasium`\n",
    "\n",
    "`$ pip install gymnasium[toy-text]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fc2673",
   "metadata": {},
   "source": [
    "Dodatno bomo potrebovali:\n",
    "\n",
    "```\n",
    "matplotlib\n",
    "tensorflow\n",
    "stable_baselines3[extra]>=2.0.0a9\n",
    "gym\n",
    "```\n",
    "\n",
    "In py `pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172d13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\").env\n",
    "env.reset(seed=123)\n",
    "env.render()\n",
    "time.sleep(3)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc94393e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc8ad645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Space Discrete(500)\n",
      "Action Space Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\").env\n",
    "env.reset(seed=123)\n",
    "env.render()\n",
    "\n",
    "print(\"State Space {}\".format(env.observation_space))\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "\n",
    "time.sleep(3)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d6521b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a8e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env.reset(seed=123)\n",
    "\n",
    "print(env.s)\n",
    "print(env.P[env.s])\n",
    "env.render()\n",
    "\n",
    "time.sleep(3)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb0bec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac11c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 5\n",
    "env.reset(seed=123)\n",
    "\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "actions = [1, 3, 3, 1, 1, 4, 0, 0, 2, 2, 2, 2, 1, 1, 5] # shortest path for seed 123\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = actions[steps]    \n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "\n",
    "env.render()\n",
    "time.sleep(2)\n",
    "env.close()\n",
    "\n",
    "print(\"Steps taken\", steps)\n",
    "print(\"Total reward\", total_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf5fb9",
   "metadata": {},
   "source": [
    "## Baseline - Random walking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0212e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 200\n",
    "env.reset(seed=123)\n",
    "\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "\n",
    "    rewards = env.P[env.s]\n",
    "    max_reward = max(rewards.values(), key=lambda x: x[0][2])[0][2] # find max reward in current state\n",
    "    best_actions = {action:outcome for action, outcome in rewards.items() if outcome[0][2]==max_reward} # collect all actions that will give this max reward\n",
    "    action = random.choice(list(best_actions.keys())) # select a random action out of these best actions\n",
    "    \n",
    "    observation, reward, done, truncated, info = env.step(action) # do the action\n",
    "    \n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "\n",
    "env.render()\n",
    "time.sleep(2)\n",
    "env.close()\n",
    "\n",
    "print(\"Steps taken\", steps)\n",
    "print(\"Total reward\", total_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef04319",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ad2f3",
   "metadata": {},
   "source": [
    "$new \\ Q(state, action) = (1 - \\alpha) * Q(state, action) + \\alpha(reward + \\gamma maxQ(next \\ state, all \\ actions))$\n",
    "\n",
    "* $Q(state, action)$ - q-value za state v katerem se trenutno nahajamo in akcijo katero bomo naredili\n",
    "* $\\alpha$ - predstavlja learning rate ($0 < \\alpha \\le 1$). Predstavlja za koliko želimo posodobiti novi q-value\n",
    "* $reward$ - reward katerega smo prejeli, ko smo opravili naš action\n",
    "* $\\gamma$ - predstavlja **discount factor**. Ta nam pove koliko pomembnosti dajemo na prihodnjo nagrado oziroma Q vrednost. Vrednost blizu 1 pomeni, da želimo povdariti končni rezultat. Vrednosti proti 0 pomeni, da želimo gledati le trenutno nagrado (greedy policy).\n",
    "* $maxQ(next \\ state, all \\ actions)$ - pogledamo v katerem stanju se bomo znašli po naši akciji. Nato vzememo največjo q-value za tiste `state-action` pare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd341d4",
   "metadata": {},
   "source": [
    "### Koda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d902fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 200\n",
    "\n",
    "# Q-Table\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# Performance tracking\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "# Env setup\n",
    "done = False\n",
    "state, info = env.reset(seed=123)\n",
    "while not done:\n",
    "    env.render()\n",
    "\n",
    "    if random.uniform(0,1) <= epsilon:\n",
    "        action = env.action_space.sample() # Explore by making random action\n",
    "    else:\n",
    "        action = np.argmax(q_table[state]) # Be greedy and make best action\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    print(\"State\", state, \"Action\", action, \"Reward\", reward, \"Next state\", next_state, \"Done\", done, \"Turncated\", truncated, \"Step\", steps) if steps % 100 == 0 else None\n",
    "\n",
    "    # Update Q-Table\n",
    "    old_value = q_table[state, action]\n",
    "    next_max = np.max(q_table[next_state])\n",
    "    new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "    q_table[state, action] = new_value\n",
    "\n",
    "\n",
    "    # Update performance tracking\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "\n",
    "env.render()\n",
    "time.sleep(2)\n",
    "env.close()\n",
    "\n",
    "print(\"Steps taken\", steps)\n",
    "print(\"Total reward\", total_reward)\n",
    "print(q_table[341])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a217f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2f62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 200\n",
    "\n",
    "# Q-Table\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# Performance tracking\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "# Env setup\n",
    "done = False\n",
    "state, info = env.reset(seed=123)\n",
    "while not done:\n",
    "    env.render()\n",
    "\n",
    "    if random.uniform(0,1) <= epsilon:\n",
    "        action = env.action_space.sample() # Explore by making random action\n",
    "    else:\n",
    "        action = np.argmax(q_table[state]) # Be greedy and make best action\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    print(\"State\", state, \"Action\", action, \"Reward\", reward, \"Next state\", next_state, \"Done\", done, \"Turncated\", truncated, \"Step\", steps) if steps % 20 == 0 else None\n",
    "    # vvvv    HERE HERE HERE    vvvvv\n",
    "    if truncated:\n",
    "        done = True\n",
    "        reward = -20\n",
    "    # ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "    # Update Q-Table\n",
    "    old_value = q_table[state, action]\n",
    "    next_max = np.max(q_table[next_state])\n",
    "    new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "    q_table[state, action] = new_value\n",
    "\n",
    "\n",
    "    # Update performance tracking\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "\n",
    "env.render()\n",
    "time.sleep(2)\n",
    "env.close()\n",
    "\n",
    "print(\"Steps taken\", steps)\n",
    "print(\"Total reward\", total_reward)\n",
    "print(q_table[341])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e7ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd23323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=None)\n",
    "env.metadata[\"render_fps\"] = 300\n",
    "\n",
    "# Q-Table\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# vvvv    HERE HERE HERE    vvvv\n",
    "episodes = 10_001\n",
    "total_rewards = []\n",
    "total_steps = []\n",
    "for ep in range(episodes):\n",
    "    # ^^^^    HERE HERE HERE    ^^^^\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    # Env setup\n",
    "    done = False\n",
    "    state, info = env.reset()  # (seed=123)\n",
    "    while not done:\n",
    "        if random.uniform(0, 1) <= epsilon:\n",
    "            action = env.action_space.sample()  # Explore by making random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Be greedy and make best action\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        if truncated:\n",
    "            done = True\n",
    "            reward = -20\n",
    "\n",
    "        # Update Q-Table\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "        state = next_state\n",
    "\n",
    "        # Update performance tracking\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "    # vvvv    HERE HERE HERE    vvvv\n",
    "    total_rewards.append(total_reward)\n",
    "    total_steps.append(steps)\n",
    "\n",
    "    if ep % 200 == 0:\n",
    "        print(\"Episode\", ep)\n",
    "        print(\"Steps taken\", steps)\n",
    "        print(\"Total reward\", total_reward)\n",
    "        print()\n",
    "\n",
    "env.close()\n",
    "print(q_table[341])\n",
    "np.save(\"models/taxi.npy\", q_table)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "ax1.plot(total_rewards[::50], c=\"lightskyblue\")\n",
    "ax1.set_ylabel(\"Rewards\")\n",
    "\n",
    "ax2.plot(total_steps[::50], c=\"pink\")\n",
    "ax2.set_xlabel(\"Episode\")\n",
    "ax2.set_ylabel(\"Steps\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "# ^^^^    HERE HERE HERE    ^^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bf3151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26249ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 5\n",
    "\n",
    "# Q-Table\n",
    "q_table = np.load(\"models/taxi_good.npy\")\n",
    "\n",
    "episodes = 1\n",
    "\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    # Env setup\n",
    "    done = False\n",
    "    state, info = env.reset(seed=123)\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])  # Be greedy and make best action\n",
    "\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        if truncated:\n",
    "            done = True\n",
    "            reward = -20\n",
    "\n",
    "        # Update performance tracking\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "    print(\"Episode\", ep)\n",
    "    print(\"Steps taken\", steps)\n",
    "    print(\"Total reward\", total_reward)\n",
    "    print()\n",
    "\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2581950e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7682797a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33758fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "q_table = np.load(\"models/taxi_good.npy\")\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(q_table, cmap=\"hot\", interpolation=\"none\", aspect=\"auto\")\n",
    "plt.colorbar(im)\n",
    "plt.title(\"Q-Table\")\n",
    "\n",
    "# Add column names to the heatmap\n",
    "column_names = [\"Down\", \"Up\", \"Right\", \"Left\", \"Pickup\", \"Dropoff\"]\n",
    "ax.set_xticks(np.arange(len(column_names)))\n",
    "ax.set_xticklabels(column_names, rotation=45)\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a16cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e48bc16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fdef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 5\n",
    "\n",
    "# Q-Table\n",
    "q_table = np.load(\"models/taxi_good.npy\")\n",
    "\n",
    "# vvvv    HERE HERE HERE    vvvv\n",
    "# Holds all states taxi visites and all neighboring states\n",
    "states = set()\n",
    "# ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "# Env setup\n",
    "done = False\n",
    "state, info = env.reset(seed=123)\n",
    "while not done:\n",
    "    # vvvv    HERE HERE HERE    vvvv\n",
    "    states.update([state])  # Add the state taxi is in\n",
    "    neigbour_states = [\n",
    "        v[0][1] for v in env.P[state].values()\n",
    "    ]  # grab neighboring states\n",
    "    states.update(neigbour_states)\n",
    "    # ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "    action = np.argmax(q_table[state])  # Be greedy and make best action\n",
    "\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    if truncated:\n",
    "        done = True\n",
    "        reward = -20\n",
    "\n",
    "env.render()\n",
    "env.close()\n",
    "\n",
    "# vvvv    HERE HERE HERE    vvvv\n",
    "print(\"States:\", list(states))\n",
    "# ^^^^    HERE HERE HERE    ^^^^\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c848da15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de855c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "states = [1, 257, 137, 397, 17, 21, 277, 157, 421, 37, 297, 177, 441, 321, 197, 201, 77, 337, 341, 85, 217, 221, 97, 101, 357, 377, 237, 241, 117, 121]\n",
    "\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 5\n",
    "\n",
    "# Q-Table\n",
    "q_table = np.load(\"models/taxi_good.npy\")\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(q_table, cmap=\"hot\", interpolation=\"none\", aspect=\"auto\")\n",
    "plt.colorbar(im)\n",
    "plt.title(\"Q-Table\")\n",
    "# Add column names to the heatmap\n",
    "column_names = [\"Down\", \"Up\", \"Right\", \"Left\", \"Pickup\", \"Dropoff\"]\n",
    "ax.set_xticks(np.arange(len(column_names)))\n",
    "ax.set_xticklabels(column_names, rotation=45)\n",
    "# Display the initial heatmap\n",
    "plt.show(block=False)\n",
    "\n",
    "# Env setup\n",
    "done = False\n",
    "state, info = env.reset(seed=123)\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])  # Be greedy and make best action\n",
    "\n",
    "    matrix_ = q_table.copy()\n",
    "    matrix_[state, action] = np.max(q_table)\n",
    "    matrix_ = matrix_[states, :]  # select only our path states\n",
    "    im.set_data(matrix_)\n",
    "    plt.pause(0.5)\n",
    "\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    if truncated:\n",
    "        done = True\n",
    "        reward = -20\n",
    "\n",
    "env.render()\n",
    "env.close()\n",
    "\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c15d9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a146d8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=None)\n",
    "env.metadata[\"render_fps\"] = 300\n",
    "\n",
    "# Q-Table\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "\n",
    "states = [\n",
    "    1,\n",
    "    257,\n",
    "    137,\n",
    "    397,\n",
    "    17,\n",
    "    21,\n",
    "    277,\n",
    "    157,\n",
    "    421,\n",
    "    37,\n",
    "    297,\n",
    "    177,\n",
    "    441,\n",
    "    321,\n",
    "    197,\n",
    "    201,\n",
    "    77,\n",
    "    337,\n",
    "    341,\n",
    "    85,\n",
    "    217,\n",
    "    221,\n",
    "    97,\n",
    "    101,\n",
    "    357,\n",
    "    377,\n",
    "    237,\n",
    "    241,\n",
    "    117,\n",
    "    121,\n",
    "]\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(q_table, cmap=\"hot\", interpolation=\"none\", aspect=\"auto\")\n",
    "plt.colorbar(im)\n",
    "plt.title(\"Q-Table\")\n",
    "# Add column names to the heatmap\n",
    "column_names = [\"Down\", \"Up\", \"Right\", \"Left\", \"Pickup\", \"Dropoff\"]\n",
    "ax.set_xticks(np.arange(len(column_names)))\n",
    "ax.set_xticklabels(column_names, rotation=45)\n",
    "# Display the initial heatmap\n",
    "plt.show(block=False)\n",
    "\n",
    "episodes = 10_001\n",
    "total_rewards = []\n",
    "total_steps = []\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    # Env setup\n",
    "    done = False\n",
    "    state, info = env.reset(seed=123)\n",
    "    while not done:\n",
    "        if random.uniform(0, 1) <= epsilon:\n",
    "            action = env.action_space.sample()  # Explore by making random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Be greedy and make best action\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        if truncated:\n",
    "            done = True\n",
    "            reward = -20\n",
    "\n",
    "        # Update Q-Table\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "        state = next_state\n",
    "\n",
    "        # Update performance tracking\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "    total_rewards.append(total_reward)\n",
    "    total_steps.append(steps)\n",
    "\n",
    "    if ep % 200 == 0:\n",
    "        print(\"Episode\", ep)\n",
    "        print(\"Steps taken\", steps)\n",
    "        print(\"Total reward\", total_reward)\n",
    "        print()\n",
    "        matrix_ = q_table.copy()\n",
    "        matrix_ = matrix_[states, :]  # select only our path states\n",
    "        im.set_data(matrix_)\n",
    "        plt.pause(0.2)\n",
    "\n",
    "\n",
    "env.close()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5aa68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9f08fbb",
   "metadata": {},
   "source": [
    "# Deep Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b27809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80c13704",
   "metadata": {},
   "source": [
    "## CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5d1ad4",
   "metadata": {},
   "source": [
    "[CartPole-v1](https://gymnasium.farama.org/environments/classic_control/cart_pole/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ab79c6",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cartpole.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeed151",
   "metadata": {},
   "source": [
    "### Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7596095",
   "metadata": {},
   "source": [
    "### Observation Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f3e9d0",
   "metadata": {},
   "source": [
    "### Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc4570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 24\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    env.reset()\n",
    "    while not done:\n",
    "        next_state, reward, done, truncated, info = env.step(env.action_space.sample())\n",
    "        if truncated:\n",
    "            done = True\n",
    "\n",
    "        # Performance tracking\n",
    "        total_reward += reward\n",
    "    \n",
    "    print(\"Episode: \", ep)\n",
    "    print(\"Total reward: \", total_reward)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561ebeea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa5a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 24\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # Hyperparameters\n",
    "        self.epsilon = 0.1\n",
    "\n",
    "        self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(32, input_dim=self.env.observation_space.shape[0], activation=\"relu\"))\n",
    "        self.model.add(Dense(16, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.env.action_space.n, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n",
    "\n",
    "    def action(self, state):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "agent = DQNAgent(env)\n",
    "print(agent.model.summary())\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    state, _ = env.reset()\n",
    "    while not done:\n",
    "\n",
    "        state = np.reshape(state, [1, env.observation_space.shape[0]])\n",
    "        action = agent.action(state)\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        if truncated:\n",
    "            done = True\n",
    "\n",
    "        # Performance tracking\n",
    "        total_reward += reward\n",
    "    \n",
    "    print(\"Episode: \", ep)\n",
    "    print(\"Total reward: \", total_reward)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8598d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=None)\n",
    "env.metadata[\"render_fps\"] = 200\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        # Hyperparameters\n",
    "        self.epsilon = 0.1\n",
    "        # vvvv    HERE HERE HERE    vvvv\n",
    "        self.gamma = 0.95\n",
    "        # ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "        self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(32, input_dim=self.env.observation_space.shape[0], activation=\"relu\"))\n",
    "        self.model.add(Dense(16, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.env.action_space.n, activation=\"linear\")) # Try something else than linear\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n",
    "\n",
    "    def action(self, state):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "    \n",
    "    # vvvv    HERE HERE HERE    vvvv\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        batch_size = 64\n",
    "\n",
    "        if len(self.memory) < batch_size:\n",
    "            return None # We don't have enough data to train\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        state = np.zeros((batch_size, self.env.observation_space.shape[0]))\n",
    "        next_state = np.zeros((batch_size, self.env.observation_space.shape[0]))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])\n",
    "\n",
    "        target = self.model.predict(state)\n",
    "        target_next = self.model.predict(next_state)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.gamma * np.amax(target_next[i])\n",
    "\n",
    "        self.model.fit(state, target, batch_size=batch_size, verbose=0)\n",
    "    # ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "\n",
    "\n",
    "agent = DQNAgent(env)\n",
    "print(agent.model.summary())\n",
    "\n",
    "episodes = 25\n",
    "# vvvv    HERE HERE HERE    vvvv\n",
    "total_rewards = []\n",
    "# ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, env.observation_space.shape[0]])\n",
    "\n",
    "    while not done:\n",
    "        action = agent.action(state)\n",
    "        \n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, env.observation_space.shape[0]])\n",
    "        # vvvv    HERE HERE HERE    vvvv\n",
    "        if done:\n",
    "            # If agent lost give big negative reward, because default reward is 1\n",
    "            reward = -10\n",
    "        elif truncated:\n",
    "            # Don't give negative reward, but stop playing so we don't have infinite episode\n",
    "            done = True\n",
    "       \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        agent.replay()\n",
    "        # ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "        # Performance tracking\n",
    "        total_reward += reward\n",
    "    \n",
    "    total_rewards.append(total_reward)\n",
    "    print(\"Episode: \", ep)\n",
    "    print(\"Total reward: \", total_reward)\n",
    "\n",
    "\n",
    "env.close()\n",
    "# vvvv    HERE HERE HERE vvvv\n",
    "agent.model.save(\"models/cartpole.h5\")\n",
    "\n",
    "# Plot total rewards\n",
    "plt.plot(total_rewards)\n",
    "plt.show()\n",
    "# ^^^^    HERE HERE HERE    ^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eeec75",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./images/cartpole_learning.png\" width=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563907bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 30\n",
    "\n",
    "agent = load_model(\"models/cartpole_good.h5\")\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, env.observation_space.shape[0]])\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(agent.predict(state)[0])        \n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        state = np.reshape(state, [1, env.observation_space.shape[0]])\n",
    "        \n",
    "        if truncated:\n",
    "            # Don't give negative reward, but stop playing so we don't have infinite episode\n",
    "            done = True\n",
    "        \n",
    "        # Performance tracking\n",
    "        total_reward += reward\n",
    "    \n",
    "    print(\"Episode: \", ep)\n",
    "    print(\"Total reward: \", total_reward)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3706970a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772307d",
   "metadata": {},
   "source": [
    "# Stable baseline 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb0934",
   "metadata": {},
   "source": [
    "Installing:\n",
    "* `pip install stable-baselines3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "model = DQN(\"MlpPolicy\", \"CartPole-v1\").learn(100_000, progress_bar=True)\n",
    "model.save(\"models/DQN_SB3\")\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 30\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    while not done:\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        if truncated:\n",
    "            done = True\n",
    "        # Performance tracking\n",
    "        total_reward += reward\n",
    "\n",
    "    print(\"Episode: \", ep)\n",
    "    print(\"Total reward: \", total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c2832f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee523d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 30\n",
    "\n",
    "model = DQN.load(\"models/DQN_SB3_Best\")\n",
    "\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    while not done:\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        if truncated:\n",
    "            done = True\n",
    "        # Performance tracking\n",
    "        total_reward += reward\n",
    "\n",
    "    print(\"Episode: \", ep)\n",
    "    print(\"Total reward: \", total_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2b5849",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa16ec6c",
   "metadata": {},
   "source": [
    "# Drugi algoritmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c98a0bf",
   "metadata": {},
   "source": [
    "![rl algos](./images/rl_algos_classification.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1170b4",
   "metadata": {},
   "source": [
    "## Model-based vs. Model-free"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b959ae1",
   "metadata": {},
   "source": [
    "## Policy learning vs. Value learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a8fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO(\"MlpPolicy\", \"CartPole-v1\").learn(10_000)\n",
    "model.save(\"models/ppo\")\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 30\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    while not done:\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        if truncated:\n",
    "            done = True\n",
    "        # Performance tracking\n",
    "        total_reward += reward\n",
    "\n",
    "    print(\"Episode: \", ep)\n",
    "    print(\"Total reward: \", total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75eb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d294ec12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1996234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import DQN, A2C, PPO\n",
    "\n",
    "#model = DQN(\"MlpPolicy\", \"CartPole-v1\").learn(10_000, progress_bar=True)\n",
    "model = A2C(\"MlpPolicy\", \"CartPole-v1\").learn(10_000, progress_bar=True)\n",
    "#model = PPO(\"MlpPolicy\", \"CartPole-v1\").learn(10_000, progress_bar=True)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 30\n",
    "\n",
    "episodes = 5\n",
    "total_rewards = []\n",
    "\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    while not done:\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        if truncated:\n",
    "            done = True\n",
    "        # Performance tracking\n",
    "        total_reward += reward\n",
    "\n",
    "    total_rewards.append(total_reward)\n",
    "    print(\"Episode: \", ep)\n",
    "    print(\"Total reward: \", total_reward)\n",
    "\n",
    "print(\"Average reward: \", sum(total_rewards) / len(total_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4740d168",
   "metadata": {},
   "source": [
    "<table class=\"docutils align-default\">\n",
    "<thead>\n",
    "<tr class=\"row-odd\"><th class=\"head\"><p>Name</p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">Box</span></code></p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">Discrete</span></code></p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">MultiDiscrete</span></code></p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">MultiBinary</span></code></p></th>\n",
    "<th class=\"head\"><p>Multi Processing</p></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"row-even\"><td><p>ARS <a class=\"footnote-reference brackets\" href=\"#f1\" id=\"id1\" role=\"doc-noteref\"><span class=\"fn-bracket\">[</span>1<span class=\"fn-bracket\">]</span></a></p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>A2C</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>DDPG</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>DQN</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>HER</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>PPO</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>QR-DQN <a class=\"footnote-reference brackets\" href=\"#f1\" id=\"id2\" role=\"doc-noteref\"><span class=\"fn-bracket\">[</span>1<span class=\"fn-bracket\">]</span></a></p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>️ ✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>RecurrentPPO <a class=\"footnote-reference brackets\" href=\"#f1\" id=\"id3\" role=\"doc-noteref\"><span class=\"fn-bracket\">[</span>1<span class=\"fn-bracket\">]</span></a></p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>SAC</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>TD3</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>TQC <a class=\"footnote-reference brackets\" href=\"#f1\" id=\"id4\" role=\"doc-noteref\"><span class=\"fn-bracket\">[</span>1<span class=\"fn-bracket\">]</span></a></p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>TRPO  <a class=\"footnote-reference brackets\" href=\"#f1\" id=\"id5\" role=\"doc-noteref\"><span class=\"fn-bracket\">[</span>1<span class=\"fn-bracket\">]</span></a></p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Maskable PPO <a class=\"footnote-reference brackets\" href=\"#f1\" id=\"id6\" role=\"doc-noteref\"><span class=\"fn-bracket\">[</span>1<span class=\"fn-bracket\">]</span></a></p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69403e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601749f1",
   "metadata": {},
   "source": [
    "# Custom Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/TheAILearner/Snake-Game-using-OpenCV-Python/blob/master/snake_game_using_opencv.ipynb\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "def collision_with_apple(apple_position, score):\n",
    "    apple_position = [random.randrange(1, 50) * 10, random.randrange(1, 50) * 10]\n",
    "    score += 1\n",
    "    return apple_position, score\n",
    "\n",
    "\n",
    "def collision_with_boundaries(snake_head):\n",
    "    if (\n",
    "        snake_head[0] >= 500\n",
    "        or snake_head[0] < 0\n",
    "        or snake_head[1] >= 500\n",
    "        or snake_head[1] < 0\n",
    "    ):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def collision_with_self(snake_position):\n",
    "    snake_head = snake_position[0]\n",
    "    if snake_head in snake_position[1:]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "# Initial Snake and Apple position\n",
    "snake_position = [[250, 250], [240, 250], [230, 250]]\n",
    "apple_position = [random.randrange(1, 50) * 10, random.randrange(1, 50) * 10]\n",
    "\n",
    "score = 0\n",
    "prev_button_direction = 1\n",
    "button_direction = 1\n",
    "\n",
    "snake_head = [250, 250]\n",
    "while True:\n",
    "    cv2.imshow(\"a\", img)\n",
    "    cv2.waitKey(1)\n",
    "    img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "    # Display Apple\n",
    "    cv2.rectangle(\n",
    "        img,\n",
    "        (apple_position[0], apple_position[1]),\n",
    "        (apple_position[0] + 10, apple_position[1] + 10),\n",
    "        (0, 0, 255),\n",
    "        3,\n",
    "    )\n",
    "    # Display Snake\n",
    "    for position in snake_position:\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            (position[0], position[1]),\n",
    "            (position[0] + 10, position[1] + 10),\n",
    "            (0, 255, 0),\n",
    "            3,\n",
    "        )\n",
    "\n",
    "    # Takes step after fixed time\n",
    "    t_end = time.time() + 0.05\n",
    "    k = -1\n",
    "    while time.time() < t_end:\n",
    "        if k == -1:\n",
    "            k = cv2.waitKey(1)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # 0-Left, 1-Right, 3-Up, 2-Down, q-Break\n",
    "    # a-Left, d-Right, w-Up, s-Down\n",
    "\n",
    "    if k == ord(\"a\") and prev_button_direction != 1:\n",
    "        button_direction = 0\n",
    "    elif k == ord(\"d\") and prev_button_direction != 0:\n",
    "        button_direction = 1\n",
    "    elif k == ord(\"w\") and prev_button_direction != 2:\n",
    "        button_direction = 3\n",
    "    elif k == ord(\"s\") and prev_button_direction != 3:\n",
    "        button_direction = 2\n",
    "    elif k == ord(\"q\"):\n",
    "        break\n",
    "    else:\n",
    "        button_direction = button_direction\n",
    "    prev_button_direction = button_direction\n",
    "\n",
    "    # Change the head position based on the button direction\n",
    "    if button_direction == 1:\n",
    "        snake_head[0] += 10\n",
    "    elif button_direction == 0:\n",
    "        snake_head[0] -= 10\n",
    "    elif button_direction == 2:\n",
    "        snake_head[1] += 10\n",
    "    elif button_direction == 3:\n",
    "        snake_head[1] -= 10\n",
    "\n",
    "    # Increase Snake length on eating apple\n",
    "    if snake_head == apple_position:\n",
    "        apple_position, score = collision_with_apple(apple_position, score)\n",
    "        snake_position.insert(0, list(snake_head))\n",
    "\n",
    "    else:\n",
    "        snake_position.insert(0, list(snake_head))\n",
    "        snake_position.pop()\n",
    "\n",
    "    # On collision kill the snake and print the score\n",
    "    if (\n",
    "        collision_with_boundaries(snake_head) == 1\n",
    "        or collision_with_self(snake_position) == 1\n",
    "    ):\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "        cv2.putText(\n",
    "            img,\n",
    "            \"Your Score is {}\".format(score),\n",
    "            (140, 250),\n",
    "            font,\n",
    "            1,\n",
    "            (255, 255, 255),\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "        cv2.imshow(\"a\", img)\n",
    "        cv2.waitKey(0)\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64edb144",
   "metadata": {},
   "source": [
    "```python\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, arg1, arg2, ...):\n",
    "        super().__init__()\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n",
    "        # Example for using image as input (channel-first; channel-last also works):\n",
    "        self.observation_space = spaces.Box(low=0, high=255,\n",
    "                                            shape=(N_CHANNELS, HEIGHT, WIDTH), dtype=np.uint8)\n",
    "\n",
    "    def step(self, action):\n",
    "        ...\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        ...\n",
    "        return observation, info\n",
    "\n",
    "    def render(self):\n",
    "        ...\n",
    "\n",
    "    def close(self):\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd26cbcc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a4f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "def collision_with_apple(apple_position, score):\n",
    "    apple_position = [random.randrange(1, 50) * 10, random.randrange(1, 50) * 10]\n",
    "    score += 1\n",
    "    return apple_position, score\n",
    "\n",
    "\n",
    "def collision_with_boundaries(snake_head):\n",
    "    if (\n",
    "        snake_head[0] >= 500\n",
    "        or snake_head[0] < 0\n",
    "        or snake_head[1] >= 500\n",
    "        or snake_head[1] < 0\n",
    "    ):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def collision_with_self(snake_position):\n",
    "    snake_head = snake_position[0]\n",
    "    if snake_head in snake_position[1:]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "class SnekEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SnekEnv, self).__init__()\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # Example for using image as input (channel-first; channel-last also works):\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        pass\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1514e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "def collision_with_apple(apple_position, score):\n",
    "    apple_position = [random.randrange(1, 50) * 10, random.randrange(1, 50) * 10]\n",
    "    score += 1\n",
    "    return apple_position, score\n",
    "\n",
    "\n",
    "def collision_with_boundaries(snake_head):\n",
    "    if (\n",
    "        snake_head[0] >= 500\n",
    "        or snake_head[0] < 0\n",
    "        or snake_head[1] >= 500\n",
    "        or snake_head[1] < 0\n",
    "    ):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def collision_with_self(snake_position):\n",
    "    snake_head = snake_position[0]\n",
    "    if snake_head in snake_position[1:]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "class SnekEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SnekEnv, self).__init__()\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # Example for using image as input (channel-first; channel-last also works):\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # vvvv    HERE HERE HERE    vvvv\n",
    "        self.img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "\n",
    "        # Initial Snake and Apple position\n",
    "        self.snake_position = [[250, 250], [240, 250], [230, 250]]\n",
    "        self.apple_position = [\n",
    "            random.randrange(1, 50) * 10,\n",
    "            random.randrange(1, 50) * 10,\n",
    "        ]\n",
    "\n",
    "        self.score = 0\n",
    "        self.snake_head = self.snake_position[0].copy()\n",
    "\n",
    "        self.done = False\n",
    "        self.truncated = False\n",
    "\n",
    "        observation = self.constructObservation()\n",
    "\n",
    "        return observation, {}  # observation, info\n",
    "        # ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    # vvvv    HERE HERE HERE    vvvv\n",
    "    def constructObservation(self):\n",
    "        # Used to create a current observation\n",
    "        \"\"\"Observation holds: (all 1 or 0 values)\n",
    "        * is apple to the left of head\n",
    "        * is apple to the right of head\n",
    "        * is apple above head\n",
    "        * is apple below head\n",
    "        * is wall or tail to the left of head\n",
    "        * is wall or tail to the right of head\n",
    "        * is wall or tail above head\n",
    "        * is wall or tail below head\n",
    "        \"\"\"\n",
    "        observation = []\n",
    "\n",
    "        if self.apple_position[0] < self.snake_head[0]:  # to the left\n",
    "            observation.extend([1, 0])\n",
    "        else:\n",
    "            observation.extend([0, 1])\n",
    "\n",
    "        if self.apple_position[1] < self.snake_head[1]:  # above\n",
    "            observation.extend([1, 0])\n",
    "        else:\n",
    "            observation.extend([0, 1])\n",
    "\n",
    "        positions = self.snake_position.copy()\n",
    "        head = positions[0].copy()\n",
    "\n",
    "        # Check left\n",
    "        head_left = [head[0] - 10, head[1]]\n",
    "        positions[0] = head_left\n",
    "        o = collision_with_boundaries(head_left) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check right\n",
    "        head_right = [head[0] + 10, head[1]]\n",
    "        positions[0] = head_right\n",
    "        o = collision_with_boundaries(head_right) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check above\n",
    "        head_above = [head[0], head[1] - 10]\n",
    "        positions[0] = head_above\n",
    "        o = collision_with_boundaries(head_above) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check below\n",
    "        head_below = [head[0], head[1] + 10]\n",
    "        positions[0] = head_below\n",
    "        o = collision_with_boundaries(head_below) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        observation = np.array(observation, dtype=np.float32)\n",
    "        return observation\n",
    "        # ^^^^    HERE HERE HERE    ^^^^\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc32f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import cv2\n",
    "\n",
    "\n",
    "def collision_with_apple(apple_position, score):\n",
    "    apple_position = [random.randrange(1, 50) * 10, random.randrange(1, 50) * 10]\n",
    "    score += 1\n",
    "    return apple_position, score\n",
    "\n",
    "\n",
    "def collision_with_boundaries(snake_head):\n",
    "    if (\n",
    "        snake_head[0] >= 500\n",
    "        or snake_head[0] < 0\n",
    "        or snake_head[1] >= 500\n",
    "        or snake_head[1] < 0\n",
    "    ):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def collision_with_self(snake_position):\n",
    "    snake_head = snake_position[0]\n",
    "    if snake_head in snake_position[1:]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "class SnekEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SnekEnv, self).__init__()\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # Example for using image as input (channel-first; channel-last also works):\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "\n",
    "        # Initial Snake and Apple position\n",
    "        self.snake_position = [[250, 250], [240, 250], [230, 250]]\n",
    "        self.apple_position = [\n",
    "            random.randrange(1, 50) * 10,\n",
    "            random.randrange(1, 50) * 10,\n",
    "        ]\n",
    "\n",
    "        self.score = 0\n",
    "        self.snake_head = self.snake_position[0].copy()\n",
    "\n",
    "        self.done = False\n",
    "        self.truncated = False\n",
    "\n",
    "        observation = self.constructObservation()\n",
    "\n",
    "        return observation, {}  # observation, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        # vvvv    HERE HERE HERE    vvvv\n",
    "        cv2.imshow(\"Snake\", self.img)  # Show the current frame\n",
    "        cv2.waitKey(\n",
    "            50\n",
    "        )  # wait 50ms. Otherwise a person can't see anything because it is too fast\n",
    "\n",
    "        # Create the next frame to be rendered\n",
    "        self.img = np.zeros((500, 500, 3), dtype=\"uint8\")  # create empty board\n",
    "\n",
    "        # Display Apple\n",
    "        cv2.rectangle(\n",
    "            self.img,\n",
    "            (self.apple_position[0], self.apple_position[1]),\n",
    "            (self.apple_position[0] + 10, self.apple_position[1] + 10),\n",
    "            (0, 0, 255),\n",
    "            3,\n",
    "        )\n",
    "        # Display Snake\n",
    "        for position in self.snake_position:\n",
    "            cv2.rectangle(\n",
    "                self.img,\n",
    "                (position[0], position[1]),\n",
    "                (position[0] + 10, position[1] + 10),\n",
    "                (0, 255, 0),\n",
    "                3,\n",
    "            )\n",
    "        # ^^^^    HERE HERE HERE ^^^^\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def constructObservation(self):\n",
    "        # Used to create a current observation\n",
    "        \"\"\"Observation holds: (all 1 or 0 values)\n",
    "        * is apple to the left of head\n",
    "        * is apple to the right of head\n",
    "        * is apple above head\n",
    "        * is apple below head\n",
    "        * is wall or tail to the left of head\n",
    "        * is wall or tail to the right of head\n",
    "        * is wall or tail above head\n",
    "        * is wall or tail below head\n",
    "        \"\"\"\n",
    "        observation = []\n",
    "\n",
    "        if self.apple_position[0] < self.snake_head[0]:  # to the left\n",
    "            observation.extend([1, 0])\n",
    "        else:\n",
    "            observation.extend([0, 1])\n",
    "\n",
    "        if self.apple_position[1] < self.snake_head[1]:  # above\n",
    "            observation.extend([1, 0])\n",
    "        else:\n",
    "            observation.extend([0, 1])\n",
    "\n",
    "        positions = self.snake_position.copy()\n",
    "        head = positions[0].copy()\n",
    "\n",
    "        # Check left\n",
    "        head_left = [head[0] - 10, head[1]]\n",
    "        positions[0] = head_left\n",
    "        o = collision_with_boundaries(head_left) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check right\n",
    "        head_right = [head[0] + 10, head[1]]\n",
    "        positions[0] = head_right\n",
    "        o = collision_with_boundaries(head_right) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check above\n",
    "        head_above = [head[0], head[1] - 10]\n",
    "        positions[0] = head_above\n",
    "        o = collision_with_boundaries(head_above) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check below\n",
    "        head_below = [head[0], head[1] + 10]\n",
    "        positions[0] = head_below\n",
    "        o = collision_with_boundaries(head_below) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        observation = np.array(observation, dtype=np.float32)\n",
    "        return observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f0f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import cv2\n",
    "\n",
    "\n",
    "def collision_with_apple(apple_position, score):\n",
    "    apple_position = [random.randrange(1, 50) * 10, random.randrange(1, 50) * 10]\n",
    "    score += 1\n",
    "    return apple_position, score\n",
    "\n",
    "\n",
    "def collision_with_boundaries(snake_head):\n",
    "    if (\n",
    "        snake_head[0] >= 500\n",
    "        or snake_head[0] < 0\n",
    "        or snake_head[1] >= 500\n",
    "        or snake_head[1] < 0\n",
    "    ):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def collision_with_self(snake_position):\n",
    "    snake_head = snake_position[0]\n",
    "    if snake_head in snake_position[1:]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "class SnekEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SnekEnv, self).__init__()\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # Example for using image as input (channel-first; channel-last also works):\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "        # vvvv    HERE HERE HERE    vvvv\n",
    "        self.render_training = False\n",
    "        # ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "    def step(self, action):\n",
    "        # vvvv    HERE HERE HERE    vvvv\n",
    "        self.render() if self.render_training else None  # If i want to see training or not\n",
    "\n",
    "        # Change the head position based on the button direction\n",
    "        if action == 1:  # move RIGHT\n",
    "            self.snake_head[0] += 10\n",
    "        elif action == 0:  # move LEFT\n",
    "            self.snake_head[0] -= 10\n",
    "        elif action == 2:  # move DOWN\n",
    "            self.snake_head[1] += 10\n",
    "        elif action == 3:  # move UP\n",
    "            self.snake_head[1] -= 10\n",
    "\n",
    "        # Calculating reward\n",
    "        self.reward = 0\n",
    "\n",
    "        # Increase Snake length on eating apple\n",
    "        if self.snake_head == self.apple_position:\n",
    "            self.apple_position, self.score = collision_with_apple(\n",
    "                self.apple_position, self.score\n",
    "            )\n",
    "            self.snake_position.insert(0, list(self.snake_head))\n",
    "        else:\n",
    "            self.snake_position.insert(0, list(self.snake_head))\n",
    "            self.snake_position.pop()\n",
    "\n",
    "        # On collision kill the snake and print the score\n",
    "        if (\n",
    "            collision_with_boundaries(self.snake_head) == 1\n",
    "            or collision_with_self(self.snake_position) == 1\n",
    "        ):\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            self.img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "            cv2.putText(\n",
    "                self.img,\n",
    "                \"Your Score is {}\".format(self.score),\n",
    "                (140, 250),\n",
    "                font,\n",
    "                1,\n",
    "                (255, 255, 255),\n",
    "                2,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "            self.done = True  # if sneak hits itself it dies\n",
    "\n",
    "        self.reward += len(self.snake_position)\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        observation = self.constructObservation()\n",
    "\n",
    "        return (\n",
    "            observation,\n",
    "            self.reward,\n",
    "            self.done,\n",
    "            self.truncated,\n",
    "            info,\n",
    "        )  # observation, reward, terminated, truncated, info\n",
    "        # ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "\n",
    "        # Initial Snake and Apple position\n",
    "        self.snake_position = [[250, 250], [240, 250], [230, 250]]\n",
    "        self.apple_position = [\n",
    "            random.randrange(1, 50) * 10,\n",
    "            random.randrange(1, 50) * 10,\n",
    "        ]\n",
    "\n",
    "        self.score = 0\n",
    "        self.snake_head = self.snake_position[0].copy()\n",
    "\n",
    "        self.done = False\n",
    "        self.truncated = False\n",
    "\n",
    "        observation = self.constructObservation()\n",
    "\n",
    "        return observation, {}  # observation, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        cv2.imshow(\"Snake\", self.img)  # Show the current frame\n",
    "        cv2.waitKey(\n",
    "            50\n",
    "        )  # wait 50ms. Otherwise a person can't see anything because it is too fast\n",
    "\n",
    "        # Create the next frame to be rendered\n",
    "        self.img = np.zeros((500, 500, 3), dtype=\"uint8\")  # create empty board\n",
    "\n",
    "        # Display Apple\n",
    "        cv2.rectangle(\n",
    "            self.img,\n",
    "            (self.apple_position[0], self.apple_position[1]),\n",
    "            (self.apple_position[0] + 10, self.apple_position[1] + 10),\n",
    "            (0, 0, 255),\n",
    "            3,\n",
    "        )\n",
    "        # Display Snake\n",
    "        for position in self.snake_position:\n",
    "            cv2.rectangle(\n",
    "                self.img,\n",
    "                (position[0], position[1]),\n",
    "                (position[0] + 10, position[1] + 10),\n",
    "                (0, 255, 0),\n",
    "                3,\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def constructObservation(self):\n",
    "        # Used to create a current observation\n",
    "        \"\"\"Observation holds: (all 1 or 0 values)\n",
    "        * is apple to the left of head\n",
    "        * is apple to the right of head\n",
    "        * is apple above head\n",
    "        * is apple below head\n",
    "        * is wall or tail to the left of head\n",
    "        * is wall or tail to the right of head\n",
    "        * is wall or tail above head\n",
    "        * is wall or tail below head\n",
    "        \"\"\"\n",
    "        observation = []\n",
    "\n",
    "        if self.apple_position[0] < self.snake_head[0]:  # to the left\n",
    "            observation.extend([1, 0])\n",
    "        else:\n",
    "            observation.extend([0, 1])\n",
    "\n",
    "        if self.apple_position[1] < self.snake_head[1]:  # above\n",
    "            observation.extend([1, 0])\n",
    "        else:\n",
    "            observation.extend([0, 1])\n",
    "\n",
    "        positions = self.snake_position.copy()\n",
    "        head = positions[0].copy()\n",
    "\n",
    "        # Check left\n",
    "        head_left = [head[0] - 10, head[1]]\n",
    "        positions[0] = head_left\n",
    "        o = collision_with_boundaries(head_left) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check right\n",
    "        head_right = [head[0] + 10, head[1]]\n",
    "        positions[0] = head_right\n",
    "        o = collision_with_boundaries(head_right) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check above\n",
    "        head_above = [head[0], head[1] - 10]\n",
    "        positions[0] = head_above\n",
    "        o = collision_with_boundaries(head_above) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check below\n",
    "        head_below = [head[0], head[1] + 10]\n",
    "        positions[0] = head_below\n",
    "        o = collision_with_boundaries(head_below) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        observation = np.array(observation, dtype=np.float32)\n",
    "        return observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "from snakeenv import SnekEnv\n",
    "\n",
    "\n",
    "env = SnekEnv()\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c59923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snakeenv import SnekEnv\n",
    "\n",
    "env = SnekEnv()\n",
    "\n",
    "episodes = 5\n",
    "for ep in range(episodes):\n",
    "    env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        observation, reward, done, _, info = env.step(env.action_space.sample())\n",
    "\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "    env.render()\n",
    "    print(\"Steps taken\", steps)\n",
    "    print(\"Total reward\", total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fbbfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from snakeenv import SnekEnv\n",
    "\n",
    "env = SnekEnv()\n",
    "policy = \"MlpPolicy\"\n",
    "model_name = \"snek\"\n",
    "\n",
    "try:\n",
    "    model = PPO.load(f\"./models/11111{model_name}\", env)\n",
    "except FileNotFoundError as e:\n",
    "    model = PPO(policy, env)\n",
    "\n",
    "model.learn(3_000, progress_bar=True)\n",
    "model.save(f\"models/{model_name}\")\n",
    "\n",
    "done = False\n",
    "obs, _ = env.reset()\n",
    "while not done:\n",
    "    env.render()\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    if truncated:\n",
    "        done = True\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916aa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from snakeenv2 import SnekEnv\n",
    "\n",
    "env = SnekEnv()\n",
    "policy = \"MlpPolicy\"\n",
    "model_name = \"snek_V1\"\n",
    "env.render_training = False\n",
    "train = True\n",
    "\n",
    "try:\n",
    "    model = PPO.load(f\"./models/{model_name}\", env)\n",
    "except FileNotFoundError as e:\n",
    "    model = PPO(policy, env)\n",
    "\n",
    "model.learn(3_000, progress_bar=True) if train else None\n",
    "model.save(f\"models/{model_name}\") if train else None\n",
    "\n",
    "done = False\n",
    "obs, _ = env.reset()\n",
    "while not done:\n",
    "    env.render()\n",
    "\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    if truncated:\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b5a683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598617f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import cv2\n",
    "\n",
    "EP_LENGTH = 300\n",
    "\n",
    "\n",
    "def collision_with_apple(apple_position, score):\n",
    "    apple_position = [random.randrange(1, 50) * 10, random.randrange(1, 50) * 10]\n",
    "    score += 1\n",
    "    return apple_position, score\n",
    "\n",
    "\n",
    "def collision_with_boundaries(snake_head):\n",
    "    if (\n",
    "        snake_head[0] >= 500\n",
    "        or snake_head[0] < 0\n",
    "        or snake_head[1] >= 500\n",
    "        or snake_head[1] < 0\n",
    "    ):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def collision_with_self(snake_position):\n",
    "    snake_head = snake_position[0]\n",
    "    if snake_head in snake_position[1:]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "class SnekEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SnekEnv, self).__init__()\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # Example for using image as input (channel-first; channel-last also works):\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "        self.render_training = False\n",
    "\n",
    "    def step(self, action):\n",
    "        self.render() if self.render_training else None  # If i want to see training or not\n",
    "\n",
    "        # Change the head position based on the button direction\n",
    "        if action == 1:  # move RIGHT\n",
    "            self.snake_head[0] += 10\n",
    "        elif action == 0:  # move LEFT\n",
    "            self.snake_head[0] -= 10\n",
    "        elif action == 2:  # move DOWN\n",
    "            self.snake_head[1] += 10\n",
    "        elif action == 3:  # move UP\n",
    "            self.snake_head[1] -= 10\n",
    "        self.ep_steps += 1\n",
    "\n",
    "        # Calculating reward\n",
    "        self.reward = 0\n",
    "\n",
    "        prev_apple_dist = np.sqrt(\n",
    "            (self.apple_position[0] - self.snake_position[1][0]) ** 2\n",
    "            + (self.apple_position[1] - self.snake_position[1][1]) ** 2\n",
    "        )\n",
    "        head_apple_dist = np.sqrt(\n",
    "            (self.apple_position[0] - self.snake_head[0]) ** 2\n",
    "            + (self.apple_position[1] - self.snake_head[1]) ** 2\n",
    "        )  # current head distance to apple\n",
    "\n",
    "        if head_apple_dist < prev_apple_dist:\n",
    "            self.reward += 1\n",
    "        else:\n",
    "            self.reward -= 1\n",
    "\n",
    "        # Increase Snake length on eating apple\n",
    "        if self.snake_head == self.apple_position:\n",
    "            self.apple_position, self.score = collision_with_apple(\n",
    "                self.apple_position, self.score\n",
    "            )\n",
    "            self.snake_position.insert(0, list(self.snake_head))\n",
    "            self.reward += 100\n",
    "        else:\n",
    "            self.snake_position.insert(0, list(self.snake_head))\n",
    "            self.snake_position.pop()\n",
    "\n",
    "        # On collision kill the snake and print the score\n",
    "        if (\n",
    "            collision_with_boundaries(self.snake_head) == 1\n",
    "            or collision_with_self(self.snake_position) == 1\n",
    "        ):\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            self.img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "            cv2.putText(\n",
    "                self.img,\n",
    "                \"Your Score is {}\".format(self.score),\n",
    "                (140, 250),\n",
    "                font,\n",
    "                1,\n",
    "                (255, 255, 255),\n",
    "                2,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "            self.done = True  # if sneak hits itself it dies\n",
    "            self.reward -= 1_000\n",
    "            print(\"Suicide\")\n",
    "\n",
    "        self.reward += len(self.snake_position)\n",
    "\n",
    "        if self.ep_steps >= EP_LENGTH:\n",
    "            self.truncated = True\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        observation = self.constructObservation()\n",
    "\n",
    "        return (\n",
    "            observation,\n",
    "            self.reward,\n",
    "            self.done,\n",
    "            self.truncated,\n",
    "            info,\n",
    "        )  # observation, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.ep_steps = 0  # how many steps we made since last reset\n",
    "        self.img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "\n",
    "        # Initial Snake and Apple position\n",
    "        self.snake_position = [[250, 250], [240, 250], [230, 250]]\n",
    "        self.apple_position = [\n",
    "            random.randrange(1, 50) * 10,\n",
    "            random.randrange(1, 50) * 10,\n",
    "        ]\n",
    "\n",
    "        self.score = 0\n",
    "        self.snake_head = self.snake_position[0].copy()\n",
    "\n",
    "        self.done = False\n",
    "        self.truncated = False\n",
    "\n",
    "        observation = self.constructObservation()\n",
    "\n",
    "        return observation, {}  # observation, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        cv2.imshow(\"Snake\", self.img)  # Show the current frame\n",
    "        cv2.waitKey(\n",
    "            50\n",
    "        )  # wait 50ms. Otherwise a person can't see anything because it is too fast\n",
    "\n",
    "        # Create the next frame to be rendered\n",
    "        self.img = np.zeros((500, 500, 3), dtype=\"uint8\")  # create empty board\n",
    "\n",
    "        # Display Apple\n",
    "        cv2.rectangle(\n",
    "            self.img,\n",
    "            (self.apple_position[0], self.apple_position[1]),\n",
    "            (self.apple_position[0] + 10, self.apple_position[1] + 10),\n",
    "            (0, 0, 255),\n",
    "            3,\n",
    "        )\n",
    "        # Display Snake\n",
    "        for position in self.snake_position:\n",
    "            cv2.rectangle(\n",
    "                self.img,\n",
    "                (position[0], position[1]),\n",
    "                (position[0] + 10, position[1] + 10),\n",
    "                (0, 255, 0),\n",
    "                3,\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def constructObservation(self):\n",
    "        # Used to create a current observation\n",
    "        \"\"\"Observation holds: (all 1 or 0 values)\n",
    "        * is apple to the left of head\n",
    "        * is apple to the right of head\n",
    "        * is apple above head\n",
    "        * is apple below head\n",
    "        * is wall or tail to the left of head\n",
    "        * is wall or tail to the right of head\n",
    "        * is wall or tail above head\n",
    "        * is wall or tail below head\n",
    "        \"\"\"\n",
    "        observation = []\n",
    "\n",
    "        if self.apple_position[0] < self.snake_head[0]:  # to the left\n",
    "            observation.extend([1, 0])\n",
    "        else:\n",
    "            observation.extend([0, 1])\n",
    "\n",
    "        if self.apple_position[1] < self.snake_head[1]:  # above\n",
    "            observation.extend([1, 0])\n",
    "        else:\n",
    "            observation.extend([0, 1])\n",
    "\n",
    "        positions = self.snake_position.copy()\n",
    "        head = positions[0].copy()\n",
    "\n",
    "        # Check left\n",
    "        head_left = [head[0] - 10, head[1]]\n",
    "        positions[0] = head_left\n",
    "        o = collision_with_boundaries(head_left) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check right\n",
    "        head_right = [head[0] + 10, head[1]]\n",
    "        positions[0] = head_right\n",
    "        o = collision_with_boundaries(head_right) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check above\n",
    "        head_above = [head[0], head[1] - 10]\n",
    "        positions[0] = head_above\n",
    "        o = collision_with_boundaries(head_above) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check below\n",
    "        head_below = [head[0], head[1] + 10]\n",
    "        positions[0] = head_below\n",
    "        o = collision_with_boundaries(head_below) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        observation = np.array(observation, dtype=np.float32)\n",
    "        return observation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
