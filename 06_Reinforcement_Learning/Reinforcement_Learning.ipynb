{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d83707a2",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37afd45c",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Cilj **deep reinforcement learning** je najti optimalne korake za pridobitev največje možne nagrade v danem okolju.\n",
    "    \n",
    "Zadevo je najlažje razložiti na primeru robota, ki igra šah (oziroma neko drugo igro). V tem primeru imamo:\n",
    "* **agent** - \"oseba\" oziroma entiteta, ki izvaja akcije\n",
    "* **action** - možne akcije, ki jih agent lahko izvede. Na primer premik figure, itd\n",
    "* **enviornment** - okoje v katerem agent deluje. V našem primeru je to šahovska plošča. Znotraj okolja imamo informacije o pozicijah različnih figure, kako se različne figure premikajo, itd.\n",
    "    \n",
    "Cilj agenta je zmagati igro šaha. Agent sedaj premika figure. Za vsak premik dobi informacijo o tem ali je bila poteza dobre ali slaba. Če je zajel nasprotnikovo figuro, je to bila dobra poteza in agent bi prejel pozitiven **reward**. Če je agent izgubil figuro, bi to bila slaba poteza in bi prejel negativn **reward**. Obstajajo pa tudi poteze, ki ne zajamejo figur. Ampak so del naše taktike s katero želimo v prihodnjih potezah zajeti figure. Cilj našega modela je, da se nauči optimalne strategije s katero pridobi največjo možno nagrado.\n",
    "    \n",
    "Ena izmed razlik med Reinforcement Learning in ostalimi machine learning algoritmi je, da imamo pri reinforcement learning-u zakasnele nagrade. To pomeni, da v določenih primerih agent izvede večje število akcij predno dobi katerokoli nagrado. Se pravi, da pri odločanju agent potrebuje upoštevati tudi možne prihodnje nagrade, ne samo trenutno posledice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43156707",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Reinforcement learning se uporablja v industrijah kot so:\n",
    "* Self-driving vehicles, kjer se lahko avto nauči planiranje poti, parkiranje, prehitevanje, itd.\n",
    "* Google DeepMind je uporabil reinforcement learning za optimizacijo hlajenja data centrov, kar je privedlo do 40% zmanjšanja stroškov. AI vsakih 5min naredi snapshot stanja data centra, AI se nato odloči za določeno akcijo, ki bo privedla do optimalne porabe. [Google DeepMind RL cooling](https://www.deepmind.com/blog/safety-first-ai-for-autonomous-data-centre-cooling-and-industrial-control)  \n",
    "* trading and finances, kjer se RL model lahko odloči za določeno akcijo (buy or sell)\n",
    "* robotika [Učenje robota metati](https://www.youtube.com/watch?v=-O-E1nFm6-A)\n",
    "* natural language processing, \n",
    "* marketing and advertisement, ...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46baafa0",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Uporabljajo se tudi v gaming industriji sam nam igre dajejo perfekten playing-ground. \n",
    "    \n",
    "Igre nudijo obstoječa kompleksna okolja, ki niso preobsežna. Če želimo, da naš robot pobere objekt v realnem svetu more sprocesirati ogromne količine senzoričnih podatkov.\n",
    "    \n",
    "Zanimiv primer je prišel od OpenAI, ki so ustvarili profesionalne bot-e za igro Dota 2 - [LINK](https://www.youtube.com/watch?v=UZHTNBMAfAA). Boti so bili tako dobro, da so premagali takratke World Champions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39026037",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d2423",
   "metadata": {},
   "source": [
    "# Taxi game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e933363",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Poglejmo si osnovne principe na primeru igre.\n",
    "    \n",
    "Simulirali bomo **self-driving taxi**. Cilj taxija, da pobere potnika na eni lokaciji in ga odloži na drugi lokaciji. Za naš taxi želimo, da:\n",
    "* odloži potnika na pravilni lokaciji\n",
    "* izbere najkrajšo pot\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd72f95",
   "metadata": {},
   "source": [
    "[Taxi_game documentation](https://gymnasium.farama.org/environments/toy_text/taxi/#taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b456d0f",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/taxi_game.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5888353d",
   "metadata": {},
   "source": [
    "## State space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ae46ea",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Pri RL se agent znajde v nekem stanju (**state**) in opravi **action**. Ta action ga premakne v naslednji **state**. Znotraj **stata** bi morali najti informacije na podlagi katerih se agent odloči za določeno akcijo - pozicija taxija, pozicija potnika, itd.\n",
    "    \n",
    "**State space** je set vseh možnih situacij v katerih bi se lahko naš taxi znašel.\n",
    "    \n",
    "Naše polje imamo razdeljeno na **5**x**5** grid, kar nam da 25 različnih lokacij v katerih se taxi lahko nahaja. Lokacija na sliki je (3, 1).\n",
    "    \n",
    "Poleg teh lokacij imamo **4** različne pozicije na katerih taxi pobere oziroma dostavi ljudi\n",
    "* R - (0, 0)\n",
    "* G - (0, 4)\n",
    "* Y - (4, 0)\n",
    "* B - (4, 3)\n",
    "Naš potnik se nahaja na lokaciji R (0, 0) in želi priti na Z (0, 4).\n",
    "    \n",
    "Dodatno moramo upoštevati stanje potinka, ki se lahko nahaja ali na izmed **4-ih** postaj ali **v** taxiju. Tako imamo dodatnih **5** stanj za potnika.\n",
    "    \n",
    "Skupaj imamo 5 * 5 * 4 * 5 = 500 vseh možnih stanj.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86719fde",
   "metadata": {},
   "source": [
    "## Action space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc692c2",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Taxi (oziroma agent) se bo znašel v nekem stanju in se odločil za določeno akcijo. Na voljo ima 6 možnosti:\n",
    "* dol\n",
    "* gor\n",
    "* desno\n",
    "* levo\n",
    "* pickup\n",
    "* dropoff\n",
    "    \n",
    "To je naš action space - nabor vseh akcij katere lahko opravimo.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29d1ba",
   "metadata": {},
   "source": [
    "## Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd826f6",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Ko taxi opravi določeno akcijo je premaknjen v novo stanje. Poleg tega prejme določen **reward**. Njegov cilj je maximizirat dosežene nagrade.\n",
    "    \n",
    "Mi mu želimo dati visoko nagrado za uspešno dostavljenega potnika. Če poizkusi odložiti potinka na napačni lokaciji mu želimo dati visok negativen reward (oziroma kazen). Dodatno mu želimo dodeliti majhno negatvno kazen za vsak korak katerega opravi. S tem ga želimo prisiliti, da opravi najkrajšo pot.\n",
    "\n",
    "Dodatno mu bomo dodelili majhno negativno nagrado, če bo taxi želel prečkati steno.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4728c13a",
   "metadata": {},
   "source": [
    "## Implementacija v Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940115cb",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Naše okolje lahko najdemo znotraj python knjižnjice Gymnasium - https://gymnasium.farama.org/\n",
    "Knjižnjica je še nedolgo nazaj obstajala kot knjižnjica OpenAI Gym.\n",
    "    \n",
    "Gymnasium nam nudi API wrapper za veliko različnih iger.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea777cb",
   "metadata": {},
   "source": [
    "`$ pip install gymnasium`\n",
    "\n",
    "oziroma\n",
    "\n",
    "`$ pip install gymnasium[toy-text]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002fe447",
   "metadata": {},
   "source": [
    "Dodatno bomo potrebovali:\n",
    "\n",
    "```\n",
    "gymnasium[toy-text]\n",
    "matplotlib\n",
    "tensorflow\n",
    "stable_baselines3[extra]>=2.0.0a9\n",
    "gym\n",
    "```\n",
    "\n",
    "in še pytorch (inštalacija odvisna od OS in našega račnalnika).\n",
    "`$ pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8e49b3",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Glavni interface s katerim delamo je `env` in predstavlja naše okolje.\n",
    "    \n",
    "* Z `env.reset()` metodo resetiramo okolje in ga postavimo v začetni položaj\n",
    "* Z `env.render()` metodo prikažemo trenutni frame\n",
    "* Z `env.close()` metodo na varen način ugasnemo naše okolje\n",
    "    \n",
    "    > We are using the .env on the end of make to avoid training stopping at 200 iterations, which is the default for the new version of Gym\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172d13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\").env\n",
    "env.reset(seed=123)\n",
    "env.render()\n",
    "time.sleep(3)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01719ad",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Znotraj atributov `observation_space` in `action_space` lahko vidimo, da imamo 500 različnih **states** in 6 različnih akcij.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc8ad645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Space Discrete(500)\n",
      "Action Space Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\").env\n",
    "env.reset(seed=123)\n",
    "env.render()\n",
    "\n",
    "print(\"State Space {}\".format(env.observation_space))\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "\n",
    "time.sleep(3)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff58b7",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Vsak **state** ima unique številko, ki ga identificira. V našem primeru na sliki je to state **341**. Poleg tega lahko za vsak state vidimo kolikšen je reward za posamezno akcijo.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a8e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env.reset(seed=123)\n",
    "\n",
    "print(env.s)\n",
    "print(env.P[env.s])\n",
    "env.render()\n",
    "\n",
    "time.sleep(3)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a15c6da",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "```python\n",
    "{0: [(1.0, 441, -1, False)], \n",
    " 1: [(1.0, 241, -1, False)], \n",
    " 2: [(1.0, 341, -1, False)], \n",
    " 3: [(1.0, 321, -1, False)], \n",
    " 4: [(1.0, 341, -10, False)], \n",
    " 5: [(1.0, 341, -10, False)]}\n",
    "```\n",
    "`{action: [(probability, nextstate, reward, done)]}`\n",
    "\n",
    "* 0-5 predstavlja naše akcije\n",
    "* probability - če se odločimo za to akcijo, kolikšna je možnost za premik v naslednji state. V Taxi primeru je to vedno 1\n",
    "* nextstate - state v katerem bomo po opravljeni akciji\n",
    "* reward - kolikšen reward prejmemo za našo akcijo\n",
    "* done - ali bo ta akcija končala episodo, ali smo potnika odložili na pravi lokaciji\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd42693",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Če bi igrali mi, bi najkrajša pot bila sledeča:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac11c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 5\n",
    "env.reset(seed=123)\n",
    "\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "actions = [1, 3, 3, 1, 1, 4, 0, 0, 2, 2, 2, 2, 1, 1, 5] # shortest path for seed 123\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = actions[steps]    \n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "\n",
    "env.render()\n",
    "time.sleep(2)\n",
    "env.close()\n",
    "\n",
    "print(\"Steps taken\", steps)\n",
    "print(\"Total reward\", total_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf5fb9",
   "metadata": {},
   "source": [
    "## Baseline - Random walking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79c2aea",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Za začetek bomo ustvarili baseline model. Ustvarili bomo neskončno zanko, kjer konstantno gledamo kolikšen je kakšen reward za kakšno akcijo. Taxi se odloči za največjo nagrado. Če jih je več istih se bo taxi odločil za random akcijo.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0212e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 200\n",
    "env.reset(seed=123)\n",
    "\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "\n",
    "    rewards = env.P[env.s]\n",
    "    max_reward = max(rewards.values(), key=lambda x: x[0][2])[0][2] # find max reward in current state\n",
    "    best_actions = {action:outcome for action, outcome in rewards.items() if outcome[0][2]==max_reward} # collect all actions that will give this max reward\n",
    "    action = random.choice(list(best_actions.keys())) # select a random action out of these best actions\n",
    "    \n",
    "    observation, reward, done, truncated, info = env.step(action) # do the action\n",
    "    \n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "\n",
    "env.render()\n",
    "time.sleep(2)\n",
    "env.close()\n",
    "\n",
    "print(\"Steps taken\", steps)\n",
    "print(\"Total reward\", total_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd570790",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Končni rezultat zelo variira ampak imamo nek baseline katerega želimo izboljšati.\n",
    "    \n",
    "```\n",
    "Total steps:  1222.2\n",
    "Total reward:  -1201.2\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef04319",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e9c89",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Q-Learning omogoča agentu, da se preko nagrad in s časom nauči katere so akcije, ki skupaj vodijo do maximalne nagrade.\n",
    "    \n",
    "Za agenta bomo ustvarili **Q-Table** v kateri imamo kot vrstice vse možne **state** in kot stolpce vse možne **action**. Vsaka celica v tabeli drži **q-value**, ki nam pove kako kvalitetna je specifična akcija v specifičnem **state**. \n",
    "    \n",
    "Na primer, ko Taxi pobere potnika tehnično ne dobi nobene nagrade. Vendar pa je ta akcija kvalitetna, saj Taxi ne more zmagati, če potnika ne pripelje na cilj. Q-value \"poberi potnika\" bi tako bila višja kot ostale akcije v tistem **state**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05161218",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Q-value se izračuna glede na enačbo:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ad2f3",
   "metadata": {},
   "source": [
    "$new \\ Q(state, action) = (1 - \\alpha) * Q(state, action) + \\alpha(reward + \\gamma maxQ(next \\ state, all \\ actions))$\n",
    "\n",
    "* $Q(state, action)$ - q-value za state v katerem se trenutno nahajamo in akcijo katero bomo naredili\n",
    "* $\\alpha$ - predstavlja learning rate ($0 < \\alpha \\le 1$). Predstavlja za koliko želimo posodobiti novi q-value\n",
    "* $reward$ - reward katerega smo prejeli, ko smo opravili naš action\n",
    "* $\\gamma$ - predstavlja **discount factor**. Ta nam pove koliko pomembnosti dajemo na prihodnjo nagrado oziroma Q vrednost. Vrednost blizu 1 pomeni, da želimo povdariti končni rezultat. Vrednosti proti 0 pomeni, da želimo gledati le trenutno nagrado (greedy policy).\n",
    "* $maxQ(next \\ state, all \\ actions)$ - pogledamo v katerem stanju se bomo znašli po naši akciji. Nato vzememo največjo q-value za tiste `state-action` pare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb8aad",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Q-value posodobimo glede na to kolikšna je trenutna vrednost in kolikšen je max q-value stanja v katerem se bomo znašli.\n",
    "    \n",
    "S takim posodabljanjem bo Taxi počasi našel optimalno pot katero mora izvesti.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865c11a9",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca25de8",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Na tak način bi se Taxi lahko overfittal in vedno izbiral samo eno in isto pot. Zato bomo dodali še faktor $\\epsilon$ s katerim se bomo občasno odločili za \"ne-optimalno\" akcijo in tako raziskovali naš enviornment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57196ab7",
   "metadata": {},
   "source": [
    "<div class=\"to_do\">\n",
    "\n",
    "Dodat primer brez epsiolona pa pol dodamo epsilon naknadno ko vidmo da ne izbere optimalne poti?\n",
    "Al je pa to preveč povdarka na qlearningu.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd341d4",
   "metadata": {},
   "source": [
    "### Koda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5e8ed2",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Q-table bomo inicializirali s samimi 0. Nato bomo začeli naš igro. S pomočjo `random.uniform(0,1)` bomo dobili random številko med 0 in 1. Če je številka manjša ali enaka epsilon bomo naredili random potezo.\n",
    "    \n",
    "Nato sledi posodobitev naše q-tabele.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d902fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 200\n",
    "\n",
    "# Q-Table\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# Performance tracking\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "# Env setup\n",
    "done = False\n",
    "state, info = env.reset(seed=123)\n",
    "while not done:\n",
    "    env.render()\n",
    "\n",
    "    if random.uniform(0,1) <= epsilon:\n",
    "        action = env.action_space.sample() # Explore by making random action\n",
    "    else:\n",
    "        action = np.argmax(q_table[state]) # Be greedy and make best action\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    print(\"State\", state, \"Action\", action, \"Reward\", reward, \"Next state\", next_state, \"Done\", done, \"Turncated\", truncated, \"Step\", steps) if steps % 100 == 0 else None\n",
    "\n",
    "    # Update Q-Table\n",
    "    old_value = q_table[state, action]\n",
    "    next_max = np.max(q_table[next_state])\n",
    "    new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "    q_table[state, action] = new_value\n",
    "\n",
    "\n",
    "    # Update performance tracking\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "\n",
    "env.render()\n",
    "time.sleep(2)\n",
    "env.close()\n",
    "\n",
    "print(\"Steps taken\", steps)\n",
    "print(\"Total reward\", total_reward)\n",
    "print(q_table[341])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9572e881",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Naš taxi se vrti v krogih in se ne uči zares. Dodajmo mu še veliko negativno nagrado, če igre ne zaključi v max 200 korakih.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2f62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 200\n",
    "\n",
    "# Q-Table\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# Performance tracking\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "# Env setup\n",
    "done = False\n",
    "state, info = env.reset(seed=123)\n",
    "while not done:\n",
    "    env.render()\n",
    "\n",
    "    if random.uniform(0,1) <= epsilon:\n",
    "        action = env.action_space.sample() # Explore by making random action\n",
    "    else:\n",
    "        action = np.argmax(q_table[state]) # Be greedy and make best action\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    print(\"State\", state, \"Action\", action, \"Reward\", reward, \"Next state\", next_state, \"Done\", done, \"Turncated\", truncated, \"Step\", steps) if steps % 20 == 0 else None\n",
    "    # vvvv    HERE HERE HERE    vvvvv\n",
    "    if truncated:\n",
    "        done = True\n",
    "        reward = -20\n",
    "    # ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "    # Update Q-Table\n",
    "    old_value = q_table[state, action]\n",
    "    next_max = np.max(q_table[next_state])\n",
    "    new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "    q_table[state, action] = new_value\n",
    "\n",
    "\n",
    "    # Update performance tracking\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "\n",
    "env.render()\n",
    "time.sleep(2)\n",
    "env.close()\n",
    "\n",
    "print(\"Steps taken\", steps)\n",
    "print(\"Total reward\", total_reward)\n",
    "print(q_table[341])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87be04d7",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Za naš primer smo na koncu izpisali q-values za našo začetno pozicijo - se pravi, katera akcija se nam najbolj izplača čisto na začetku.\n",
    "    \n",
    "`[-0.9948844  -0.9953616  -0.99496515 -2.8953616  -3.439      -4.0951    ]`\n",
    "    \n",
    "Glede na končno q-tabelo vidimo da so najboljše 3 akcije `down`, `up`, `right`. Vidimo, da se ni zares nekaj naučil. Želeli bi le eno zares dobro akcijo.\n",
    "    \n",
    "Smo pa taxi učili samo z eno episodo. Poizkusimo ga učiti tekom več igranj igre.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd23323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "# vvvv    HERE HERE HERE    vvvv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=None)\n",
    "# ^^^^    HERE HERE HERE    ^^^^\n",
    "env.metadata[\"render_fps\"] = 300\n",
    "\n",
    "# Q-Table\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# vvvv    HERE HERE HERE    vvvv\n",
    "episodes = 10_001\n",
    "total_rewards = []\n",
    "total_steps = []\n",
    "for ep in range(episodes):\n",
    "    # ^^^^    HERE HERE HERE    ^^^^\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    # Env setup\n",
    "    done = False\n",
    "    state, info = env.reset()  # (seed=123)\n",
    "    while not done:\n",
    "        # vvvv    HERE HERE HERE    vvvv\n",
    "        # env.render() # we dont need to render anything\n",
    "        # ^^^^    HERE HERE HERE    ^^^^\n",
    "        if random.uniform(0, 1) <= epsilon:\n",
    "            action = env.action_space.sample()  # Explore by making random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Be greedy and make best action\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        if truncated:\n",
    "            done = True\n",
    "            reward = -20\n",
    "\n",
    "        # Update Q-Table\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "        state = next_state\n",
    "\n",
    "        # Update performance tracking\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "    # vvvv    HERE HERE HERE    vvvv\n",
    "    total_rewards.append(total_reward)\n",
    "    total_steps.append(steps)\n",
    "\n",
    "    if ep % 200 == 0:\n",
    "        print(\"Episode\", ep)\n",
    "        print(\"Steps taken\", steps)\n",
    "        print(\"Total reward\", total_reward)\n",
    "        print()\n",
    "\n",
    "env.close()\n",
    "print(q_table[341])\n",
    "np.save(\"models/taxi.npy\", q_table)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "ax1.plot(total_rewards[::50], c=\"lightskyblue\")\n",
    "ax1.set_ylabel(\"Rewards\")\n",
    "\n",
    "ax2.plot(total_steps[::50], c=\"pink\")\n",
    "ax2.set_xlabel(\"Episode\")\n",
    "ax2.set_ylabel(\"Steps\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "# ^^^^    HERE HERE HERE    ^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb8419f",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Po daljšem učenju dobimo sledeč rezultat glede najboljše začetne akcije:\n",
    "    \n",
    "`[ -2.6029108   -3.49428052  -2.66963255  -2.48236823 -11.49844851   -11.64223247]`\n",
    "    \n",
    "Tukaj vidimo, da je optimalna akcija `left`, kar je ena izmed optimalnih rešitev. \n",
    "    \n",
    "Na naših grafih vidimo, da se je celotna nagrada izboljševala in, da je čas igranja upadal (kar pomeni, da se taxi ni vrtel v krogih ampak je dostavil potnika).\n",
    "    \n",
    "Če si pogledamo našo episodo sedaj:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26249ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 5\n",
    "\n",
    "# Q-Table\n",
    "q_table = np.load(\"models/taxi_good.npy\")\n",
    "\n",
    "episodes = 1\n",
    "\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    # Env setup\n",
    "    done = False\n",
    "    state, info = env.reset(seed=123)\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])  # Be greedy and make best action\n",
    "\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        if truncated:\n",
    "            done = True\n",
    "            reward = -20\n",
    "\n",
    "        # Update performance tracking\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "    print(\"Episode\", ep)\n",
    "    print(\"Steps taken\", steps)\n",
    "    print(\"Total reward\", total_reward)\n",
    "    print()\n",
    "\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1dd5c9",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Bolj optimalno kot to nebi mogl met.\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396339e1",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">    \n",
    "V našem primeru si lahko še pogledamo heat-map naše Q-Tabele. Višji Q-Value kot imamo, bolj bela je naša barva. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51830582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "q_table = np.load(\"models/taxi_good.npy\")\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(q_table, cmap=\"hot\", interpolation=\"none\", aspect=\"auto\")\n",
    "plt.colorbar(im)\n",
    "plt.title(\"Q-Table\")\n",
    "\n",
    "# Add column names to the heatmap\n",
    "column_names = [\"Down\", \"Up\", \"Right\", \"Left\", \"Pickup\", \"Dropoff\"]\n",
    "ax.set_xticks(np.arange(len(column_names)))\n",
    "ax.set_xticklabels(column_names, rotation=45)\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990c590c",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">    \n",
    "\n",
    "Zadeva je rahlo nepregledna zato vzemimo le vsa stanja v katerih se taxi znajde in vsa sosednja stanja.\n",
    "    \n",
    "Prvo pridobimo vsa stanja katera taxi obišče in vsa sosednja stanja.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583c3a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 5\n",
    "\n",
    "# Q-Table\n",
    "q_table = np.load(\"models/taxi_good.npy\")\n",
    "\n",
    "# vvvv    HERE HERE HERE    vvvv\n",
    "# Holds all states taxi visites and all neighboring states\n",
    "states = set()\n",
    "# ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "# Env setup\n",
    "done = False\n",
    "state, info = env.reset(seed=123)\n",
    "while not done:\n",
    "    # vvvv    HERE HERE HERE    vvvv\n",
    "    states.update([state])  # Add the state taxi is in\n",
    "    neigbour_states = [\n",
    "        v[0][1] for v in env.P[state].values()\n",
    "    ]  # grab neighboring states\n",
    "    states.update(neigbour_states)\n",
    "    # ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "    action = np.argmax(q_table[state])  # Be greedy and make best action\n",
    "\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    if truncated:\n",
    "        done = True\n",
    "        reward = -20\n",
    "\n",
    "env.render()\n",
    "env.close()\n",
    "\n",
    "# vvvv    HERE HERE HERE    vvvv\n",
    "print(\"States:\", list(states))\n",
    "# ^^^^    HERE HERE HERE    ^^^^\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b01382e",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">    \n",
    "\n",
    "States: `[1, 257, 137, 397, 17, 21, 277, 157, 421, 37, 297, 177, 441, 321, 197, 201, 77, 337, 341, 85, 217, 221, 97, 101, 357, 377, 237, 241, 117, 121]`\n",
    "    \n",
    "Sedaj pa si poonvon poglejmo heatmap ter kako se taxi premika po tej heat mapi.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec34798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "states = [1, 257, 137, 397, 17, 21, 277, 157, 421, 37, 297, 177, 441, 321, 197, 201, 77, 337, 341, 85, 217, 221, 97, 101, 357, 377, 237, 241, 117, 121]\n",
    "\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 5\n",
    "\n",
    "# Q-Table\n",
    "q_table = np.load(\"models/taxi_good.npy\")\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(q_table, cmap=\"hot\", interpolation=\"none\", aspect=\"auto\")\n",
    "plt.colorbar(im)\n",
    "plt.title(\"Q-Table\")\n",
    "# Add column names to the heatmap\n",
    "column_names = [\"Down\", \"Up\", \"Right\", \"Left\", \"Pickup\", \"Dropoff\"]\n",
    "ax.set_xticks(np.arange(len(column_names)))\n",
    "ax.set_xticklabels(column_names, rotation=45)\n",
    "# Display the initial heatmap\n",
    "plt.show(block=False)\n",
    "\n",
    "# Env setup\n",
    "done = False\n",
    "state, info = env.reset(seed=123)\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])  # Be greedy and make best action\n",
    "\n",
    "    matrix_ = q_table.copy()\n",
    "    matrix_[state, action] = np.max(q_table)\n",
    "    matrix_ = matrix_[states, :]  # select only our path states\n",
    "    im.set_data(matrix_)\n",
    "    plt.pause(0.5)\n",
    "\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    if truncated:\n",
    "        done = True\n",
    "        reward = -20\n",
    "\n",
    "env.render()\n",
    "env.close()\n",
    "\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8584ed9d",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">    \n",
    "Pogledamo si še lahko kako se heatmap spreminja tekom učenja.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2679af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=None)\n",
    "env.metadata[\"render_fps\"] = 300\n",
    "\n",
    "# Q-Table\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "\n",
    "states = [\n",
    "    1,\n",
    "    257,\n",
    "    137,\n",
    "    397,\n",
    "    17,\n",
    "    21,\n",
    "    277,\n",
    "    157,\n",
    "    421,\n",
    "    37,\n",
    "    297,\n",
    "    177,\n",
    "    441,\n",
    "    321,\n",
    "    197,\n",
    "    201,\n",
    "    77,\n",
    "    337,\n",
    "    341,\n",
    "    85,\n",
    "    217,\n",
    "    221,\n",
    "    97,\n",
    "    101,\n",
    "    357,\n",
    "    377,\n",
    "    237,\n",
    "    241,\n",
    "    117,\n",
    "    121,\n",
    "]\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(q_table, cmap=\"hot\", interpolation=\"none\", aspect=\"auto\")\n",
    "plt.colorbar(im)\n",
    "plt.title(\"Q-Table\")\n",
    "# Add column names to the heatmap\n",
    "column_names = [\"Down\", \"Up\", \"Right\", \"Left\", \"Pickup\", \"Dropoff\"]\n",
    "ax.set_xticks(np.arange(len(column_names)))\n",
    "ax.set_xticklabels(column_names, rotation=45)\n",
    "# Display the initial heatmap\n",
    "plt.show(block=False)\n",
    "\n",
    "episodes = 10_001\n",
    "total_rewards = []\n",
    "total_steps = []\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    # Env setup\n",
    "    done = False\n",
    "    state, info = env.reset(seed=123)\n",
    "    while not done:\n",
    "        if random.uniform(0, 1) <= epsilon:\n",
    "            action = env.action_space.sample()  # Explore by making random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Be greedy and make best action\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        if truncated:\n",
    "            done = True\n",
    "            reward = -20\n",
    "\n",
    "        # Update Q-Table\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "        state = next_state\n",
    "\n",
    "        # Update performance tracking\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "    total_rewards.append(total_reward)\n",
    "    total_steps.append(steps)\n",
    "\n",
    "    if ep % 200 == 0:\n",
    "        print(\"Episode\", ep)\n",
    "        print(\"Steps taken\", steps)\n",
    "        print(\"Total reward\", total_reward)\n",
    "        print()\n",
    "        matrix_ = q_table.copy()\n",
    "        matrix_ = matrix_[states, :]  # select only our path states\n",
    "        im.set_data(matrix_)\n",
    "        plt.pause(0.2)\n",
    "\n",
    "\n",
    "env.close()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5effa89a",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Q-Learning je eden izmed najbolj preprostih Reinforcement Learning algoritmov. Problem se pojavi, ko število **states** postane izredno veliko saj se velikost Q-Table neznansko poveča.\n",
    "    \n",
    "Za reševanje tega problema se uporablja nevronske mreže in tako dobimo **Deep Q Learning**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b92a2b",
   "metadata": {},
   "source": [
    "<div class=\"to_do\">\n",
    "\n",
    "Ta heatmap tekom učenja ni najlepše za vidt kaj se dogaja.. težko je razbrat kaj gledaš\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d4b74",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f08fbb",
   "metadata": {},
   "source": [
    "# Deep Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb2ca48",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Pri Deep Q Learning-u sedaj uporabljamo nevronsko mrežo znotraj katere hranimo Q vrednosti. Vhodne vrednosti so naš state vrednosti. Izhodne vredonsti so naše Q vrednosti. Sedaj se lahko naučimo bolj kompleksna okolja. \n",
    "    \n",
    "Deep Q Learning je sposoben tudi opraviti dobre akcije v stanjih v katerih se še ni znašel. Ve, da je že bil v podobni situaciji in tako lahko opravi bolj optimalno akcijo kot pa Q-Table.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c13704",
   "metadata": {},
   "source": [
    "## CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77aa5c5",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Za naš primer bomo ponovno uporabili knjižnjico Gymnasium. Tokrat bomo vzeli igro `CartPole-v1`.\n",
    "    \n",
    "Cilj igre je balancirati palco na premikajoči škatli.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5d1ad4",
   "metadata": {},
   "source": [
    "[CartPole-v1](https://gymnasium.farama.org/environments/classic_control/cart_pole/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ab79c6",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/cartpole.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeed151",
   "metadata": {},
   "source": [
    "### Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8609bb70",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Na voljo imamo 2 akciji:\n",
    "* 0 - premik škatle v levo\n",
    "* 1 - premik škatle v desno\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7596095",
   "metadata": {},
   "source": [
    "### Observation Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c88fb59",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Naš state je sestavljen iz 4 vrednosti:\n",
    "* 0 - `cart position` - pozicija škatle, ki se giblje med -4.8 do 4.8\n",
    "* 1 - `cart velocity` - hitrost škatle, ki se giblje med -inf in inf.\n",
    "* 2 - `pole angle` - kot palice, ki se giblje med -0.418 rad in 0.418 rad\n",
    "* 3 - `pole angular velocity` - kotna hitrost palice, ki se giblje med -inf in inf\n",
    "    \n",
    "Treba je paziti, saj ni mogoč doseči vseh stanj. Igra se zaključi, če se škatla premakne preko pozicij -2.4 oziroma +2.4. Prav tako se igra zaključi, če je kot palice večji kot -0.2095 oziroma 0.2095\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f3e9d0",
   "metadata": {},
   "source": [
    "### Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46968b67",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Cil igre je čim dlje časa balancirati palico. Za vsak korak, ki ga napravimo, dobimo nagrado `+1`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114637c6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d424ae",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Poglejmo si, če model dela random akcije.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc4570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 24\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    env.reset()\n",
    "    while not done:\n",
    "        next_state, reward, done, truncated, info = env.step(env.action_space.sample())\n",
    "        if truncated:\n",
    "            done = True\n",
    "\n",
    "        # Performance tracking\n",
    "        total_reward += reward\n",
    "    \n",
    "    print(\"Episode: \", ep)\n",
    "    print(\"Total reward: \", total_reward)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561ebeea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7a1563",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Ustvarimo sedaj DQN Agenta, ki bo opravljal random akcije - agent se še ne bo učil.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa5a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 24\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # Hyperparameters\n",
    "        self.epsilon = 0.1\n",
    "\n",
    "        self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(32, input_dim=self.env.observation_space.shape[0], activation=\"relu\"))\n",
    "        self.model.add(Dense(16, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.env.action_space.n, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n",
    "\n",
    "    def action(self, state):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "agent = DQNAgent(env)\n",
    "print(agent.model.summary())\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    state, _ = env.reset()\n",
    "    while not done:\n",
    "\n",
    "        state = np.reshape(state, [1, env.observation_space.shape[0]])\n",
    "        action = agent.action(state)\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        if truncated:\n",
    "            done = True\n",
    "\n",
    "        # Performance tracking\n",
    "        total_reward += reward\n",
    "    \n",
    "    print(\"Episode: \", ep)\n",
    "    print(\"Total reward: \", total_reward)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8598d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc675b",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Modela nočemo fit-ati na eno trenutno izkušnjo/nagrado saj tako lahko začne pozabljati, kaj vse se je že naučil. Zato bomo uporabili replay memory. To je kot nek približek biološkega spomina. V njega shranjujemo naše izkušnje iz katerih se nato lahko za nazaj učimo.\n",
    "\n",
    "To pomeni, da bomo naše states in nagrade shranjevali v nek list omejene velikost. Vsak korak bomo vzeli random batch naših stanj in naredili fit čez ta batch.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=None)\n",
    "env.metadata[\"render_fps\"] = 200\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # Hyperparameters\n",
    "        self.epsilon = 0.1\n",
    "        # vvvv    HERE HERE HERE    vvvv\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        # ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "        self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(32, input_dim=self.env.observation_space.shape[0], activation=\"relu\"))\n",
    "        self.model.add(Dense(16, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.env.action_space.n, activation=\"linear\")) # Try something else than linear\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n",
    "\n",
    "    def action(self, state):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "    \n",
    "    # vvvv    HERE HERE HERE    vvvv\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        batch_size = 64\n",
    "\n",
    "        if len(self.memory) < batch_size:\n",
    "            return None # We don't have enough data to train\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        state = np.zeros((batch_size, self.env.observation_space.shape[0]))\n",
    "        next_state = np.zeros((batch_size, self.env.observation_space.shape[0]))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])\n",
    "\n",
    "        target = self.model.predict(state)\n",
    "        target_next = self.model.predict(next_state)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.gamma * np.amax(target_next[i])\n",
    "\n",
    "        self.model.fit(state, target, batch_size=batch_size, verbose=0)\n",
    "    # ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "\n",
    "\n",
    "agent = DQNAgent(env)\n",
    "print(agent.model.summary())\n",
    "\n",
    "episodes = 25\n",
    "# vvvv    HERE HERE HERE    vvvv\n",
    "total_rewards = []\n",
    "# ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, env.observation_space.shape[0]])\n",
    "\n",
    "    while not done:\n",
    "        action = agent.action(state)\n",
    "        \n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, env.observation_space.shape[0]])\n",
    "        # vvvv    HERE HERE HERE    vvvv\n",
    "        if done:\n",
    "            # If agent lost give big negative reward, because default reward is 1\n",
    "            reward = -10\n",
    "        elif truncated:\n",
    "            # Don't give negative reward, but stop playing so we don't have infinite episode\n",
    "            done = True\n",
    "       \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        agent.replay()\n",
    "        # ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "        # Performance tracking\n",
    "        total_reward += reward\n",
    "    \n",
    "    total_rewards.append(total_reward)\n",
    "    print(\"Episode: \", ep)\n",
    "    print(\"Total reward: \", total_reward)\n",
    "\n",
    "\n",
    "env.close()\n",
    "# vvvv    HERE HERE HERE vvvv\n",
    "agent.model.save(\"models/cartpole.h5\")\n",
    "\n",
    "# Plot total rewards\n",
    "plt.plot(total_rewards)\n",
    "plt.show()\n",
    "# ^^^^    HERE HERE HERE    ^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eeec75",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./images/cartpole_learning.png\" width=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127cb98e",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Test da pogledamo zdej par episod ko igra\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563907bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 30\n",
    "\n",
    "agent = load_model(\"models/cartpole_good.h5\")\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, env.observation_space.shape[0]])\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(agent.predict(state)[0])        \n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        state = np.reshape(state, [1, env.observation_space.shape[0]])\n",
    "        \n",
    "        if truncated:\n",
    "            # Don't give negative reward, but stop playing so we don't have infinite episode\n",
    "            done = True\n",
    "        \n",
    "        # Performance tracking\n",
    "        total_reward += reward\n",
    "    \n",
    "    print(\"Episode: \", ep)\n",
    "    print(\"Total reward: \", total_reward)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3706970a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772307d",
   "metadata": {},
   "source": [
    "# Stable baseline 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bebef0b",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Da ne potrebujemo sami pisati vseh alogiritmov obstajajo za to že narejene knjižnjice.\n",
    "\n",
    "Ena izmed knjižnjic je [Stable baseline 3](https://stable-baselines.readthedocs.io/en/master/), ki znotraj sebe implementira različne algoritme in nudi podporo za Gym enviornments.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb0934",
   "metadata": {},
   "source": [
    "Installing:\n",
    "* `pip install stable-baselines3`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7572a4",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Za začetek bomo ponovili naš CartPole primer.\n",
    "\n",
    "Prvo uvozimo Gymnasium knjižnjico s katero bomo definirali okolje katerega bomo uporabili. Nato preko stable baselines knjiznjice uvozimo model katerega želimo uporabiti. V našem primeru bomo začeli z že znanim DQN. \n",
    "\n",
    "Nato ustvarimo okolje in ustvarimo agenta. Uporabili bomo `MlpPolicy` - navaden deep neural network.\n",
    "\n",
    "Nato bomo model trenirali za 100_000 korakov.\n",
    "\n",
    "Model bomo shranili in za konec bomo prikazali nekaj episodo igranja.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ba3e4",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Kaj pomeni **deterministic** parameter?\n",
    "    \n",
    "> This parameter corresponds to **\"Whether to use deterministic or stochastic actions\"**. So the thing is when you are selecting an action according to given state, the **actor_network** gives you a probability distribution. For example for two possible actions **a1** and **a2**: `[0.25, 0.75]`. If you use `deterministic=True`, the result will be action **a2** since it has more probability. In the case of `deterministic=False`, the result action will be selected with given probabilities `[0.25, 0.75]`.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "model = DQN(\"MlpPolicy\", \"CartPole-v1\").learn(100_000, progress_bar=True)\n",
    "model.save(\"models/DQN_SB3\")\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 30\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    while not done:\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        if truncated:\n",
    "            done = True\n",
    "        # Performance tracking\n",
    "        total_reward += reward\n",
    "\n",
    "    print(\"Episode: \", ep)\n",
    "    print(\"Total reward: \", total_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135d6423",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Da uporabimo shranjen model imamo preprosto load funkcijo, ki naloži `.zip` datoteko shranjenega modela.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee523d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 30\n",
    "\n",
    "model = DQN.load(\"models/DQN_SB3_Best\")\n",
    "\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    while not done:\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        if truncated:\n",
    "            done = True\n",
    "        # Performance tracking\n",
    "        total_reward += reward\n",
    "\n",
    "    print(\"Episode: \", ep)\n",
    "    print(\"Total reward: \", total_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2b5849",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa16ec6c",
   "metadata": {},
   "source": [
    "# Drugi algoritmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487df8df",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Poleg DQN imamo znotraj knjižnjice implementirane še druge algoritme.\n",
    "\n",
    "V splošnem jih je težko razdeliti v posamezne kategorije ampak v grobem se delijo na sledeč način:\n",
    "\n",
    "* **Model-Based**\n",
    "* **Model-Free**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c98a0bf",
   "metadata": {},
   "source": [
    "![rl algos](./images/rl_algos_classification.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1170b4",
   "metadata": {},
   "source": [
    "## Model-based vs. Model-free"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c844529e",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Glavno vprašanje je **ali ima agent dostop do modela okolja, oziroma se model okolja nauči**? Model okolja v tem kontekstu pomenu funkcijo, ki definira prehode med stanji in nagrade.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfd593a",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Glavna prednost, če imamo model okolja je, da ta omogoča agentu planiranje v naprej. Agent lahko vidi katere možnosti mu bodo na voljo v prihodnosti in se odloča na podlagi tega planiranja. Znan primer **model-based** algoritma je AlphaZero.\n",
    "\n",
    "Glavna ovira pri teh algoritmih je, da agent ponavadi nima dostopa do **gronud-truth modela okolja**. Kar pomeni, da če se agent želi naučiti model okolja, bo to moral doseči preko svojih izkušenj. Eden izmed problem tukaj je potencialno zelo dolg čas učenja. Drugi problem je, da se agent ne nauči pravilnega modela okolja ampak nek njegov približek, ki vsebuje določen *bias / pristranskost*. Ta bias agent izkoristi, kar privede do zelo dobri rezultatov tekom učenja, vendar pa se agent nato v realnom okolju izkaže zelo slabo.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d70002a",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "**Model-free** alogiritmi omogočajo lažjo implementacijo in lažje spreminjanje / *tuning*. Zato se večinoma uporablja te algoritme.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b959ae1",
   "metadata": {},
   "source": [
    "## Policy learning vs. Value learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a96b978",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Druga točka v kateri se razlikujejo algoritmi je **kaj naj se naučijo?**\n",
    "\n",
    "V primeru **model-free** algoritmov imamo v glavnem dve možnosti:\n",
    "* **Policy optimization** - te algoritmi direktno optimizirajo **policy funkcijo**, ki mapira stanja v akcije, ki bodo imele najvišjo končno nagrado.\n",
    "\n",
    "* **Q-learning** - te algoritmi se učijo **action-value funkcijo**, ki pove kako dobra je določena akcija\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994f7250",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "**Policy optimization** algoritmi so bolj stabilni in zanesljivi. Na drugi strani pa **Q-learning** algoritmi veliko bolje izkoriščajo podatke katere imajo na voljo.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedc0b72",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ee7b0c",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Znotraj naše knjižnjice lahko tako najdemo algoritme kot so:\n",
    "* DQN - katerega smo že uporabili\n",
    "* A2C - kjer naenkrat treniramo dve mreži. Prva predstavlja **actor** in je naš agenti, ki dela akcije. Druga mreža je **critic**, ki agentu podaja feedback glede njegovih akcij\n",
    "* PPO - kjer algoritem posodablja svoj policy v zelo majhnih korakih\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a8fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO(\"MlpPolicy\", \"CartPole-v1\").learn(10_000, progress_bar=True)\n",
    "model.save(\"models/ppo\")\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 30\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    while not done:\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        if truncated:\n",
    "            done = True\n",
    "        # Performance tracking\n",
    "        total_reward += reward\n",
    "\n",
    "    print(\"Episode: \", ep)\n",
    "    print(\"Total reward: \", total_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a7036d",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Poglejmo si primerjavo med DQN, A2C in PPO, kjer smo vsakega trenirali 10 000 korakov.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1996234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import DQN, A2C, PPO\n",
    "\n",
    "#model = DQN(\"MlpPolicy\", \"CartPole-v1\").learn(10_000, progress_bar=True)\n",
    "model = A2C(\"MlpPolicy\", \"CartPole-v1\").learn(10_000, progress_bar=True)\n",
    "#model = PPO(\"MlpPolicy\", \"CartPole-v1\").learn(10_000, progress_bar=True)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 30\n",
    "\n",
    "episodes = 5\n",
    "total_rewards = []\n",
    "\n",
    "for ep in range(episodes):\n",
    "    # Performance tracking\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    while not done:\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        if truncated:\n",
    "            done = True\n",
    "        # Performance tracking\n",
    "        total_reward += reward\n",
    "\n",
    "    total_rewards.append(total_reward)\n",
    "    print(\"Episode: \", ep)\n",
    "    print(\"Total reward: \", total_reward)\n",
    "\n",
    "print(\"Average reward: \", sum(total_rewards) / len(total_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd62efa",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Vidimo, da sta A2C in PPO dosti boljša od DQN.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7722c853",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23764faf",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Razlike med algoritmi so tudi v tem v kakšnih okoljih jih uporabljamo.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4740d168",
   "metadata": {},
   "source": [
    "<table class=\"docutils align-default\">\n",
    "<thead>\n",
    "<tr class=\"row-odd\"><th class=\"head\"><p>Name</p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">Box</span></code></p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">Discrete</span></code></p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">MultiDiscrete</span></code></p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">MultiBinary</span></code></p></th>\n",
    "<th class=\"head\"><p>Multi Processing</p></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"row-even\"><td><p>ARS <a class=\"footnote-reference brackets\" href=\"#f1\" id=\"id1\" role=\"doc-noteref\"><span class=\"fn-bracket\">[</span>1<span class=\"fn-bracket\">]</span></a></p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>A2C</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>DDPG</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>DQN</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>HER</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>PPO</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>QR-DQN <a class=\"footnote-reference brackets\" href=\"#f1\" id=\"id2\" role=\"doc-noteref\"><span class=\"fn-bracket\">[</span>1<span class=\"fn-bracket\">]</span></a></p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>️ ✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>RecurrentPPO <a class=\"footnote-reference brackets\" href=\"#f1\" id=\"id3\" role=\"doc-noteref\"><span class=\"fn-bracket\">[</span>1<span class=\"fn-bracket\">]</span></a></p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>SAC</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>TD3</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>TQC <a class=\"footnote-reference brackets\" href=\"#f1\" id=\"id4\" role=\"doc-noteref\"><span class=\"fn-bracket\">[</span>1<span class=\"fn-bracket\">]</span></a></p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>TRPO  <a class=\"footnote-reference brackets\" href=\"#f1\" id=\"id5\" role=\"doc-noteref\"><span class=\"fn-bracket\">[</span>1<span class=\"fn-bracket\">]</span></a></p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Maskable PPO <a class=\"footnote-reference brackets\" href=\"#f1\" id=\"id6\" role=\"doc-noteref\"><span class=\"fn-bracket\">[</span>1<span class=\"fn-bracket\">]</span></a></p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2691e7",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "* `Box` predstavlja vse možne akcije v določenemu action space (**continous**). Za primer lahko vzememo vožnjo avtomobila, kjer lahko na gas pritisnemo malo, močneje, zelo močno, itd.\n",
    "* `Discrete` predstavlja seznam možnih akcij, kjer lahko v določenm trenutku opravimo eno izmed akcij. To smo imeli v primeru našega taxija. V določenem trenutku som lahko naredili eno specifično akcijo.\n",
    "* `MultiDiscrete` - določena okolja imajo več seznamov diskretnih akcij. Za primer lahko vzememo upravljanje telesa. V določenem trenutku lahko premaknemo roko in/ali nogo.\n",
    "* `MultiBinary` - podobno kot `MultiDiscrete`, le da imamo za vsako akcijo 2 možnosti.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69403e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601749f1",
   "metadata": {},
   "source": [
    "# Custom Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db70cf26",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Znotraj knjiznjice Stable Baselines lahko tudi ustvarimo naš custom environment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c430bd5d",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Za naš primer bomo ustvarili igro **snake** (kača).\n",
    "    \n",
    "Kodo lahko najdemo na internetu. Za začetek samo prekopirajmo okolje in ga sami odigrajmo.\n",
    "\n",
    "Tipke so **W, S, A, D**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/TheAILearner/Snake-Game-using-OpenCV-Python/blob/master/snake_game_using_opencv.ipynb\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "def collision_with_apple(apple_position, score):\n",
    "    apple_position = [random.randrange(1, 50) * 10, random.randrange(1, 50) * 10]\n",
    "    score += 1\n",
    "    return apple_position, score\n",
    "\n",
    "\n",
    "def collision_with_boundaries(snake_head):\n",
    "    if (\n",
    "        snake_head[0] >= 500\n",
    "        or snake_head[0] < 0\n",
    "        or snake_head[1] >= 500\n",
    "        or snake_head[1] < 0\n",
    "    ):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def collision_with_self(snake_position):\n",
    "    snake_head = snake_position[0]\n",
    "    if snake_head in snake_position[1:]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "# Initial Snake and Apple position\n",
    "snake_position = [[250, 250], [240, 250], [230, 250]]\n",
    "apple_position = [random.randrange(1, 50) * 10, random.randrange(1, 50) * 10]\n",
    "\n",
    "score = 0\n",
    "prev_button_direction = 1\n",
    "button_direction = 1\n",
    "\n",
    "snake_head = [250, 250]\n",
    "while True:\n",
    "    cv2.imshow(\"a\", img)\n",
    "    cv2.waitKey(1)\n",
    "    img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "    # Display Apple\n",
    "    cv2.rectangle(\n",
    "        img,\n",
    "        (apple_position[0], apple_position[1]),\n",
    "        (apple_position[0] + 10, apple_position[1] + 10),\n",
    "        (0, 0, 255),\n",
    "        3,\n",
    "    )\n",
    "    # Display Snake\n",
    "    for position in snake_position:\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            (position[0], position[1]),\n",
    "            (position[0] + 10, position[1] + 10),\n",
    "            (0, 255, 0),\n",
    "            3,\n",
    "        )\n",
    "\n",
    "    # Takes step after fixed time\n",
    "    t_end = time.time() + 0.05\n",
    "    k = -1\n",
    "    while time.time() < t_end:\n",
    "        if k == -1:\n",
    "            k = cv2.waitKey(1)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # 0-Left, 1-Right, 3-Up, 2-Down, q-Break\n",
    "    # a-Left, d-Right, w-Up, s-Down\n",
    "\n",
    "    if k == ord(\"a\") and prev_button_direction != 1:\n",
    "        button_direction = 0\n",
    "    elif k == ord(\"d\") and prev_button_direction != 0:\n",
    "        button_direction = 1\n",
    "    elif k == ord(\"w\") and prev_button_direction != 2:\n",
    "        button_direction = 3\n",
    "    elif k == ord(\"s\") and prev_button_direction != 3:\n",
    "        button_direction = 2\n",
    "    elif k == ord(\"q\"):\n",
    "        break\n",
    "    else:\n",
    "        button_direction = button_direction\n",
    "    prev_button_direction = button_direction\n",
    "\n",
    "    # Change the head position based on the button direction\n",
    "    if button_direction == 1:\n",
    "        snake_head[0] += 10\n",
    "    elif button_direction == 0:\n",
    "        snake_head[0] -= 10\n",
    "    elif button_direction == 2:\n",
    "        snake_head[1] += 10\n",
    "    elif button_direction == 3:\n",
    "        snake_head[1] -= 10\n",
    "\n",
    "    # Increase Snake length on eating apple\n",
    "    if snake_head == apple_position:\n",
    "        apple_position, score = collision_with_apple(apple_position, score)\n",
    "        snake_position.insert(0, list(snake_head))\n",
    "\n",
    "    else:\n",
    "        snake_position.insert(0, list(snake_head))\n",
    "        snake_position.pop()\n",
    "\n",
    "    # On collision kill the snake and print the score\n",
    "    if (\n",
    "        collision_with_boundaries(snake_head) == 1\n",
    "        or collision_with_self(snake_position) == 1\n",
    "    ):\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "        cv2.putText(\n",
    "            img,\n",
    "            \"Your Score is {}\".format(score),\n",
    "            (140, 250),\n",
    "            font,\n",
    "            1,\n",
    "            (255, 255, 255),\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "        cv2.imshow(\"a\", img)\n",
    "        cv2.waitKey(0)\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fd61b9",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Našo igro moramo prvo pretvoriti v Gymnasium enviornment.\n",
    "\n",
    "To pomeni, da moramo dedovati od `gym.Env` class-a in implementirati določene metode.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64edb144",
   "metadata": {},
   "source": [
    "```python\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, arg1, arg2, ...):\n",
    "        super().__init__()\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n",
    "        # Example for using image as input (channel-first; channel-last also works):\n",
    "        self.observation_space = spaces.Box(low=0, high=255,\n",
    "                                            shape=(N_CHANNELS, HEIGHT, WIDTH), dtype=np.uint8)\n",
    "\n",
    "    def step(self, action):\n",
    "        ...\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        ...\n",
    "        return observation, info\n",
    "\n",
    "    def render(self):\n",
    "        ...\n",
    "\n",
    "    def close(self):\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd26cbcc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd310bee",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Za začetek bomo definirali naš action space in observation space.\n",
    "\n",
    "Napravimo lahko 4 diskretne akcije (ali up, ali down, ali left ali right).\n",
    "\n",
    "Kot observation space je mamljivo preprosto vzeti naši sliko. Vendar to pomeni, da se more agent dodatno naučiti kaj slika predstavlja (kaj na sliki predstavlja kačo, kaj predstavlja jabolko, itd.). Boljše je, če lahko sami definiramo določene metrike.\n",
    "\n",
    "V našem primeru bomo v enem stanju imeli podatke za:\n",
    "* is apple to the left of head\n",
    "* is apple to the right of head\n",
    "* is apple above head\n",
    "* is apple below head\n",
    "* is wall or tail to the left of head\n",
    "* is wall or tail to the right of head\n",
    "* is wall or tail above head\n",
    "* is wall or tail below head\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a4f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "def collision_with_apple(apple_position, score):\n",
    "    apple_position = [random.randrange(1, 50) * 10, random.randrange(1, 50) * 10]\n",
    "    score += 1\n",
    "    return apple_position, score\n",
    "\n",
    "\n",
    "def collision_with_boundaries(snake_head):\n",
    "    if (\n",
    "        snake_head[0] >= 500\n",
    "        or snake_head[0] < 0\n",
    "        or snake_head[1] >= 500\n",
    "        or snake_head[1] < 0\n",
    "    ):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def collision_with_self(snake_position):\n",
    "    snake_head = snake_position[0]\n",
    "    if snake_head in snake_position[1:]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "class SnekEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SnekEnv, self).__init__()\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # Example for using image as input (channel-first; channel-last also works):\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        pass\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ec4df6",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Naslednjo stvar katero bomo dodali je `reset()` metoda. Ta preprosto ponastavi naše okolje.\n",
    "    \n",
    "Dodatno bomo že sedaj pripravili funkcijo, ki nam zgradi naš observation (vrne state v katerem se nahajamo).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1514e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "def collision_with_apple(apple_position, score):\n",
    "    apple_position = [random.randrange(1, 50) * 10, random.randrange(1, 50) * 10]\n",
    "    score += 1\n",
    "    return apple_position, score\n",
    "\n",
    "\n",
    "def collision_with_boundaries(snake_head):\n",
    "    if (\n",
    "        snake_head[0] >= 500\n",
    "        or snake_head[0] < 0\n",
    "        or snake_head[1] >= 500\n",
    "        or snake_head[1] < 0\n",
    "    ):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def collision_with_self(snake_position):\n",
    "    snake_head = snake_position[0]\n",
    "    if snake_head in snake_position[1:]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "class SnekEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SnekEnv, self).__init__()\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # Example for using image as input (channel-first; channel-last also works):\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # vvvv    HERE HERE HERE    vvvv\n",
    "        self.img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "\n",
    "        # Initial Snake and Apple position\n",
    "        self.snake_position = [[250, 250], [240, 250], [230, 250]]\n",
    "        self.apple_position = [\n",
    "            random.randrange(1, 50) * 10,\n",
    "            random.randrange(1, 50) * 10,\n",
    "        ]\n",
    "\n",
    "        self.score = 0\n",
    "        self.snake_head = self.snake_position[0].copy()\n",
    "\n",
    "        self.done = False\n",
    "        self.truncated = False\n",
    "\n",
    "        observation = self.constructObservation()\n",
    "\n",
    "        return observation, {}  # observation, info\n",
    "        # ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    # vvvv    HERE HERE HERE    vvvv\n",
    "    def constructObservation(self):\n",
    "        # Used to create a current observation\n",
    "        \"\"\"Observation holds: (all 1 or 0 values)\n",
    "        * is apple to the left of head\n",
    "        * is apple to the right of head\n",
    "        * is apple above head\n",
    "        * is apple below head\n",
    "        * is wall or tail to the left of head\n",
    "        * is wall or tail to the right of head\n",
    "        * is wall or tail above head\n",
    "        * is wall or tail below head\n",
    "        \"\"\"\n",
    "        observation = []\n",
    "\n",
    "        if self.apple_position[0] < self.snake_head[0]:  # to the left\n",
    "            observation.extend([1, 0])\n",
    "        else:\n",
    "            observation.extend([0, 1])\n",
    "\n",
    "        if self.apple_position[1] < self.snake_head[1]:  # above\n",
    "            observation.extend([1, 0])\n",
    "        else:\n",
    "            observation.extend([0, 1])\n",
    "\n",
    "        positions = self.snake_position.copy()\n",
    "        head = positions[0].copy()\n",
    "\n",
    "        # Check left\n",
    "        head_left = [head[0] - 10, head[1]]\n",
    "        positions[0] = head_left\n",
    "        o = collision_with_boundaries(head_left) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check right\n",
    "        head_right = [head[0] + 10, head[1]]\n",
    "        positions[0] = head_right\n",
    "        o = collision_with_boundaries(head_right) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check above\n",
    "        head_above = [head[0], head[1] - 10]\n",
    "        positions[0] = head_above\n",
    "        o = collision_with_boundaries(head_above) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check below\n",
    "        head_below = [head[0], head[1] + 10]\n",
    "        positions[0] = head_below\n",
    "        o = collision_with_boundaries(head_below) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        observation = np.array(observation, dtype=np.float32)\n",
    "        return observation\n",
    "        # ^^^^    HERE HERE HERE    ^^^^\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed209fbd",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Nato bomo dodali `render()` metodo.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c631ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import cv2\n",
    "\n",
    "\n",
    "def collision_with_apple(apple_position, score):\n",
    "    apple_position = [random.randrange(1, 50) * 10, random.randrange(1, 50) * 10]\n",
    "    score += 1\n",
    "    return apple_position, score\n",
    "\n",
    "\n",
    "def collision_with_boundaries(snake_head):\n",
    "    if (\n",
    "        snake_head[0] >= 500\n",
    "        or snake_head[0] < 0\n",
    "        or snake_head[1] >= 500\n",
    "        or snake_head[1] < 0\n",
    "    ):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def collision_with_self(snake_position):\n",
    "    snake_head = snake_position[0]\n",
    "    if snake_head in snake_position[1:]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "class SnekEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SnekEnv, self).__init__()\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # Example for using image as input (channel-first; channel-last also works):\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "\n",
    "        # Initial Snake and Apple position\n",
    "        self.snake_position = [[250, 250], [240, 250], [230, 250]]\n",
    "        self.apple_position = [\n",
    "            random.randrange(1, 50) * 10,\n",
    "            random.randrange(1, 50) * 10,\n",
    "        ]\n",
    "\n",
    "        self.score = 0\n",
    "        self.snake_head = self.snake_position[0].copy()\n",
    "\n",
    "        self.done = False\n",
    "        self.truncated = False\n",
    "\n",
    "        observation = self.constructObservation()\n",
    "\n",
    "        return observation, {}  # observation, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        # vvvv    HERE HERE HERE    vvvv\n",
    "        cv2.imshow(\"Snake\", self.img)  # Show the current frame\n",
    "        cv2.waitKey(\n",
    "            50\n",
    "        )  # wait 50ms. Otherwise a person can't see anything because it is too fast\n",
    "\n",
    "        # Create the next frame to be rendered\n",
    "        self.img = np.zeros((500, 500, 3), dtype=\"uint8\")  # create empty board\n",
    "\n",
    "        # Display Apple\n",
    "        cv2.rectangle(\n",
    "            self.img,\n",
    "            (self.apple_position[0], self.apple_position[1]),\n",
    "            (self.apple_position[0] + 10, self.apple_position[1] + 10),\n",
    "            (0, 0, 255),\n",
    "            3,\n",
    "        )\n",
    "        # Display Snake\n",
    "        for position in self.snake_position:\n",
    "            cv2.rectangle(\n",
    "                self.img,\n",
    "                (position[0], position[1]),\n",
    "                (position[0] + 10, position[1] + 10),\n",
    "                (0, 255, 0),\n",
    "                3,\n",
    "            )\n",
    "        # ^^^^    HERE HERE HERE ^^^^\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def constructObservation(self):\n",
    "        # Used to create a current observation\n",
    "        \"\"\"Observation holds: (all 1 or 0 values)\n",
    "        * is apple to the left of head\n",
    "        * is apple to the right of head\n",
    "        * is apple above head\n",
    "        * is apple below head\n",
    "        * is wall or tail to the left of head\n",
    "        * is wall or tail to the right of head\n",
    "        * is wall or tail above head\n",
    "        * is wall or tail below head\n",
    "        \"\"\"\n",
    "        observation = []\n",
    "\n",
    "        if self.apple_position[0] < self.snake_head[0]:  # to the left\n",
    "            observation.extend([1, 0])\n",
    "        else:\n",
    "            observation.extend([0, 1])\n",
    "\n",
    "        if self.apple_position[1] < self.snake_head[1]:  # above\n",
    "            observation.extend([1, 0])\n",
    "        else:\n",
    "            observation.extend([0, 1])\n",
    "\n",
    "        positions = self.snake_position.copy()\n",
    "        head = positions[0].copy()\n",
    "\n",
    "        # Check left\n",
    "        head_left = [head[0] - 10, head[1]]\n",
    "        positions[0] = head_left\n",
    "        o = collision_with_boundaries(head_left) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check right\n",
    "        head_right = [head[0] + 10, head[1]]\n",
    "        positions[0] = head_right\n",
    "        o = collision_with_boundaries(head_right) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check above\n",
    "        head_above = [head[0], head[1] - 10]\n",
    "        positions[0] = head_above\n",
    "        o = collision_with_boundaries(head_above) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check below\n",
    "        head_below = [head[0], head[1] + 10]\n",
    "        positions[0] = head_below\n",
    "        o = collision_with_boundaries(head_below) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        observation = np.array(observation, dtype=np.float32)\n",
    "        return observation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10589308",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Nato bomo dodali `step()` metodo. Poleg observacije bomo tukaj definirali še naš reward.\n",
    "\n",
    "Kot reward bomo začeli s preprosto enačbo - reward je enak dolžini kače.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f0f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import cv2\n",
    "\n",
    "\n",
    "def collision_with_apple(apple_position, score):\n",
    "    apple_position = [random.randrange(1, 50) * 10, random.randrange(1, 50) * 10]\n",
    "    score += 1\n",
    "    return apple_position, score\n",
    "\n",
    "\n",
    "def collision_with_boundaries(snake_head):\n",
    "    if (\n",
    "        snake_head[0] >= 500\n",
    "        or snake_head[0] < 0\n",
    "        or snake_head[1] >= 500\n",
    "        or snake_head[1] < 0\n",
    "    ):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def collision_with_self(snake_position):\n",
    "    snake_head = snake_position[0]\n",
    "    if snake_head in snake_position[1:]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "class SnekEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SnekEnv, self).__init__()\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # Example for using image as input (channel-first; channel-last also works):\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "        # vvvv    HERE HERE HERE    vvvv\n",
    "        self.render_training = False\n",
    "        # ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "    def step(self, action):\n",
    "        # vvvv    HERE HERE HERE    vvvv\n",
    "        self.render() if self.render_training else None  # If i want to see training or not\n",
    "\n",
    "        # Change the head position based on the button direction\n",
    "        if action == 1:  # move RIGHT\n",
    "            self.snake_head[0] += 10\n",
    "        elif action == 0:  # move LEFT\n",
    "            self.snake_head[0] -= 10\n",
    "        elif action == 2:  # move DOWN\n",
    "            self.snake_head[1] += 10\n",
    "        elif action == 3:  # move UP\n",
    "            self.snake_head[1] -= 10\n",
    "\n",
    "        # Calculating reward\n",
    "        self.reward = 0\n",
    "\n",
    "        # Increase Snake length on eating apple\n",
    "        if self.snake_head == self.apple_position:\n",
    "            self.apple_position, self.score = collision_with_apple(\n",
    "                self.apple_position, self.score\n",
    "            )\n",
    "            self.snake_position.insert(0, list(self.snake_head))\n",
    "        else:\n",
    "            self.snake_position.insert(0, list(self.snake_head))\n",
    "            self.snake_position.pop()\n",
    "\n",
    "        # On collision kill the snake and print the score\n",
    "        if (\n",
    "            collision_with_boundaries(self.snake_head) == 1\n",
    "            or collision_with_self(self.snake_position) == 1\n",
    "        ):\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            self.img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "            cv2.putText(\n",
    "                self.img,\n",
    "                \"Your Score is {}\".format(self.score),\n",
    "                (140, 250),\n",
    "                font,\n",
    "                1,\n",
    "                (255, 255, 255),\n",
    "                2,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "            self.done = True  # if sneak hits itself it dies\n",
    "\n",
    "        self.reward += len(self.snake_position)\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        observation = self.constructObservation()\n",
    "\n",
    "        return (\n",
    "            observation,\n",
    "            self.reward,\n",
    "            self.done,\n",
    "            self.truncated,\n",
    "            info,\n",
    "        )  # observation, reward, terminated, truncated, info\n",
    "        # ^^^^    HERE HERE HERE    ^^^^\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "\n",
    "        # Initial Snake and Apple position\n",
    "        self.snake_position = [[250, 250], [240, 250], [230, 250]]\n",
    "        self.apple_position = [\n",
    "            random.randrange(1, 50) * 10,\n",
    "            random.randrange(1, 50) * 10,\n",
    "        ]\n",
    "\n",
    "        self.score = 0\n",
    "        self.snake_head = self.snake_position[0].copy()\n",
    "\n",
    "        self.done = False\n",
    "        self.truncated = False\n",
    "\n",
    "        observation = self.constructObservation()\n",
    "\n",
    "        return observation, {}  # observation, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        cv2.imshow(\"Snake\", self.img)  # Show the current frame\n",
    "        cv2.waitKey(\n",
    "            50\n",
    "        )  # wait 50ms. Otherwise a person can't see anything because it is too fast\n",
    "\n",
    "        # Create the next frame to be rendered\n",
    "        self.img = np.zeros((500, 500, 3), dtype=\"uint8\")  # create empty board\n",
    "\n",
    "        # Display Apple\n",
    "        cv2.rectangle(\n",
    "            self.img,\n",
    "            (self.apple_position[0], self.apple_position[1]),\n",
    "            (self.apple_position[0] + 10, self.apple_position[1] + 10),\n",
    "            (0, 0, 255),\n",
    "            3,\n",
    "        )\n",
    "        # Display Snake\n",
    "        for position in self.snake_position:\n",
    "            cv2.rectangle(\n",
    "                self.img,\n",
    "                (position[0], position[1]),\n",
    "                (position[0] + 10, position[1] + 10),\n",
    "                (0, 255, 0),\n",
    "                3,\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def constructObservation(self):\n",
    "        # Used to create a current observation\n",
    "        \"\"\"Observation holds: (all 1 or 0 values)\n",
    "        * is apple to the left of head\n",
    "        * is apple to the right of head\n",
    "        * is apple above head\n",
    "        * is apple below head\n",
    "        * is wall or tail to the left of head\n",
    "        * is wall or tail to the right of head\n",
    "        * is wall or tail above head\n",
    "        * is wall or tail below head\n",
    "        \"\"\"\n",
    "        observation = []\n",
    "\n",
    "        if self.apple_position[0] < self.snake_head[0]:  # to the left\n",
    "            observation.extend([1, 0])\n",
    "        else:\n",
    "            observation.extend([0, 1])\n",
    "\n",
    "        if self.apple_position[1] < self.snake_head[1]:  # above\n",
    "            observation.extend([1, 0])\n",
    "        else:\n",
    "            observation.extend([0, 1])\n",
    "\n",
    "        positions = self.snake_position.copy()\n",
    "        head = positions[0].copy()\n",
    "\n",
    "        # Check left\n",
    "        head_left = [head[0] - 10, head[1]]\n",
    "        positions[0] = head_left\n",
    "        o = collision_with_boundaries(head_left) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check right\n",
    "        head_right = [head[0] + 10, head[1]]\n",
    "        positions[0] = head_right\n",
    "        o = collision_with_boundaries(head_right) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check above\n",
    "        head_above = [head[0], head[1] - 10]\n",
    "        positions[0] = head_above\n",
    "        o = collision_with_boundaries(head_above) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check below\n",
    "        head_below = [head[0], head[1] + 10]\n",
    "        positions[0] = head_below\n",
    "        o = collision_with_boundaries(head_below) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        observation = np.array(observation, dtype=np.float32)\n",
    "        return observation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2462b13",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "`close()` metodo bomo tudi izpustili, saj ne potrebujemo narediti nobenga posebnega zaključka.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82db09a5",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Stable Defussion knjiznjica nam ponudi tudi metodo, s katero lahko okvirno potestiramo naše okolje.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "from snakeenv import SnekEnv\n",
    "\n",
    "\n",
    "env = SnekEnv()\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1999a73f",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Če ne dobimo nobene napake lahko nadaljujemo z učenjem našega modela.\n",
    "\n",
    "Prvo si bomo pogledali agenta, ki vzema random akcije:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c59923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snakeenv import SnekEnv\n",
    "\n",
    "env = SnekEnv()\n",
    "\n",
    "episodes = 5\n",
    "for ep in range(episodes):\n",
    "    env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        observation, reward, done, _, info = env.step(env.action_space.sample())\n",
    "\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "    env.render()\n",
    "    print(\"Steps taken\", steps)\n",
    "    print(\"Total reward\", total_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef52ff37",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Sedaj pa lahko treniramo agenta. Uporabili bomo PPO algoritm.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc815039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from snakeenv import SnekEnv\n",
    "\n",
    "env = SnekEnv()\n",
    "policy = \"MlpPolicy\"\n",
    "model_name = \"snek\"\n",
    "\n",
    "try:\n",
    "    model = PPO.load(f\"./models/{model_name}\", env)\n",
    "except FileNotFoundError as e:\n",
    "    model = PPO(policy, env)\n",
    "\n",
    "model.learn(3_000, progress_bar=True)\n",
    "model.save(f\"models/{model_name}\")\n",
    "\n",
    "done = False\n",
    "obs, _ = env.reset()\n",
    "while not done:\n",
    "    env.render()\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    if truncated:\n",
    "        done = True\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c170512",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Agenta smo trenirali zelo malo časa, zato se ni veliko naučil. \n",
    "    \n",
    "Za napraj nam bo tudi koristilo imeti metodo, s katero lahko prikažemo agentovo igranje tekom učenja.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f9ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from snakeenv import SnekEnv\n",
    "\n",
    "env = SnekEnv()\n",
    "policy = \"MlpPolicy\"\n",
    "model_name = \"snek_V1\"\n",
    "env.render_training = False\n",
    "train = True\n",
    "\n",
    "try:\n",
    "    model = PPO.load(f\"./models/{model_name}\", env)\n",
    "except FileNotFoundError as e:\n",
    "    model = PPO(policy, env)\n",
    "\n",
    "model.learn(3_000, progress_bar=True) if train else None\n",
    "model.save(f\"models/{model_name}\") if train else None\n",
    "\n",
    "done = False\n",
    "obs, _ = env.reset()\n",
    "while not done:\n",
    "    env.render()\n",
    "\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    if truncated:\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a288a",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Sedaj bi sledilo učenje in igranje s tem kaj naš agent \"vidi\" (**observation**) in kakšno nagrado prejme kdaj (**reward function**).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b9c7e2",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "Moje učenje je bilo sledeče:\n",
    "\n",
    "Začeli sm s to postavitvijo kot jo imamo. Učili smo 100_000 korakov.\n",
    "    \n",
    "Kača se je naučila, da naj se ne ubije. Naučila se je vrteti v krogu.\n",
    "    \n",
    "Problem je, ker kača dobi večjo nagrado šele, ko ponesreči pobere jabolko. Ker pa je plošča velika se to zgodi zelo malokrat.\n",
    "    \n",
    "(Model: `snek_V1`)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c51812",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "2. Spremenil sem reward funkcijo tako, da se je dodalo veliko negativno nagrado (-1_000), če je kača umrla. Če je kača pojedla jabolk, je dobila veliko nagrado (100). Če se je kača premaknila proti jabolku dobi +1.\n",
    "\n",
    "Treniralo se je za 100_000 korakov.\n",
    "    \n",
    "Problem je, ker je trajanje episode neskončno. Kača se lahko vrti v krogu in vsake toliko dobi točke samo zato ker se je ponesreči približala jabolku.\n",
    "\n",
    "(Model: `snek_V2`)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b5a5a",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "3. Definiralo se je maximalno trajanje episode. Episoda se konča po 200 korakih. Če se episoda tako konča, kača ne dobi nobene negativne nagrade.\n",
    "    \n",
    "Treniralo se je dodatnih 100_000 korakov.\n",
    "    \n",
    "Kača se je sedaj naučila obrniti proti jabolku in ga pojesti. Pot ni optimizirana ampak kača dela nepotrebne vijuge. Problem je, ker je nagrajena za to, da se približa jabolku, če pa se oddalji pa ne prejme nagrade.\n",
    "\n",
    "(Model: `snek_V3`)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822e303b",
   "metadata": {},
   "source": [
    "<div class=\"not_clean\">\n",
    "\n",
    "4. Nagrado se je spremenilo tako, da kača sedaj prejme -1, če se je oddaljila od jabolka.\n",
    "    \n",
    "Treniralo se je še ene 100_000 korakov.\n",
    "    \n",
    "Kača sedaj obere dokaj direktno pot do jabolka in uspešno pobira nova jabolka.\n",
    "\n",
    "(Model: `snek_V4`)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import cv2\n",
    "\n",
    "EP_LENGTH = 300\n",
    "\n",
    "\n",
    "def collision_with_apple(apple_position, score):\n",
    "    apple_position = [random.randrange(1, 50) * 10, random.randrange(1, 50) * 10]\n",
    "    score += 1\n",
    "    return apple_position, score\n",
    "\n",
    "\n",
    "def collision_with_boundaries(snake_head):\n",
    "    if (\n",
    "        snake_head[0] >= 500\n",
    "        or snake_head[0] < 0\n",
    "        or snake_head[1] >= 500\n",
    "        or snake_head[1] < 0\n",
    "    ):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def collision_with_self(snake_position):\n",
    "    snake_head = snake_position[0]\n",
    "    if snake_head in snake_position[1:]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "class SnekEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SnekEnv, self).__init__()\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # Example for using image as input (channel-first; channel-last also works):\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "        self.render_training = False\n",
    "\n",
    "    def step(self, action):\n",
    "        self.render() if self.render_training else None  # If i want to see training or not\n",
    "\n",
    "        # Change the head position based on the button direction\n",
    "        if action == 1:  # move RIGHT\n",
    "            self.snake_head[0] += 10\n",
    "        elif action == 0:  # move LEFT\n",
    "            self.snake_head[0] -= 10\n",
    "        elif action == 2:  # move DOWN\n",
    "            self.snake_head[1] += 10\n",
    "        elif action == 3:  # move UP\n",
    "            self.snake_head[1] -= 10\n",
    "        self.ep_steps += 1\n",
    "\n",
    "        # Calculating reward\n",
    "        self.reward = 0\n",
    "\n",
    "        prev_apple_dist = np.sqrt(\n",
    "            (self.apple_position[0] - self.snake_position[1][0]) ** 2\n",
    "            + (self.apple_position[1] - self.snake_position[1][1]) ** 2\n",
    "        )\n",
    "        head_apple_dist = np.sqrt(\n",
    "            (self.apple_position[0] - self.snake_head[0]) ** 2\n",
    "            + (self.apple_position[1] - self.snake_head[1]) ** 2\n",
    "        )  # current head distance to apple\n",
    "\n",
    "        if head_apple_dist < prev_apple_dist:\n",
    "            self.reward += 1\n",
    "        else:\n",
    "            self.reward -= 1\n",
    "\n",
    "        # Increase Snake length on eating apple\n",
    "        if self.snake_head == self.apple_position:\n",
    "            self.apple_position, self.score = collision_with_apple(\n",
    "                self.apple_position, self.score\n",
    "            )\n",
    "            self.snake_position.insert(0, list(self.snake_head))\n",
    "            self.reward += 100\n",
    "        else:\n",
    "            self.snake_position.insert(0, list(self.snake_head))\n",
    "            self.snake_position.pop()\n",
    "\n",
    "        # On collision kill the snake and print the score\n",
    "        if (\n",
    "            collision_with_boundaries(self.snake_head) == 1\n",
    "            or collision_with_self(self.snake_position) == 1\n",
    "        ):\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            self.img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "            cv2.putText(\n",
    "                self.img,\n",
    "                \"Your Score is {}\".format(self.score),\n",
    "                (140, 250),\n",
    "                font,\n",
    "                1,\n",
    "                (255, 255, 255),\n",
    "                2,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "            self.done = True  # if sneak hits itself it dies\n",
    "            self.reward -= 1_000\n",
    "            print(\"Suicide\")\n",
    "\n",
    "        self.reward += len(self.snake_position)\n",
    "\n",
    "        if self.ep_steps >= EP_LENGTH:\n",
    "            self.truncated = True\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        observation = self.constructObservation()\n",
    "\n",
    "        return (\n",
    "            observation,\n",
    "            self.reward,\n",
    "            self.done,\n",
    "            self.truncated,\n",
    "            info,\n",
    "        )  # observation, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.ep_steps = 0  # how many steps we made since last reset\n",
    "        self.img = np.zeros((500, 500, 3), dtype=\"uint8\")\n",
    "\n",
    "        # Initial Snake and Apple position\n",
    "        self.snake_position = [[250, 250], [240, 250], [230, 250]]\n",
    "        self.apple_position = [\n",
    "            random.randrange(1, 50) * 10,\n",
    "            random.randrange(1, 50) * 10,\n",
    "        ]\n",
    "\n",
    "        self.score = 0\n",
    "        self.snake_head = self.snake_position[0].copy()\n",
    "\n",
    "        self.done = False\n",
    "        self.truncated = False\n",
    "\n",
    "        observation = self.constructObservation()\n",
    "\n",
    "        return observation, {}  # observation, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        cv2.imshow(\"Snake\", self.img)  # Show the current frame\n",
    "        cv2.waitKey(\n",
    "            50\n",
    "        )  # wait 50ms. Otherwise a person can't see anything because it is too fast\n",
    "\n",
    "        # Create the next frame to be rendered\n",
    "        self.img = np.zeros((500, 500, 3), dtype=\"uint8\")  # create empty board\n",
    "\n",
    "        # Display Apple\n",
    "        cv2.rectangle(\n",
    "            self.img,\n",
    "            (self.apple_position[0], self.apple_position[1]),\n",
    "            (self.apple_position[0] + 10, self.apple_position[1] + 10),\n",
    "            (0, 0, 255),\n",
    "            3,\n",
    "        )\n",
    "        # Display Snake\n",
    "        for position in self.snake_position:\n",
    "            cv2.rectangle(\n",
    "                self.img,\n",
    "                (position[0], position[1]),\n",
    "                (position[0] + 10, position[1] + 10),\n",
    "                (0, 255, 0),\n",
    "                3,\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def constructObservation(self):\n",
    "        # Used to create a current observation\n",
    "        \"\"\"Observation holds: (all 1 or 0 values)\n",
    "        * is apple to the left of head\n",
    "        * is apple to the right of head\n",
    "        * is apple above head\n",
    "        * is apple below head\n",
    "        * is wall or tail to the left of head\n",
    "        * is wall or tail to the right of head\n",
    "        * is wall or tail above head\n",
    "        * is wall or tail below head\n",
    "        \"\"\"\n",
    "        observation = []\n",
    "\n",
    "        if self.apple_position[0] < self.snake_head[0]:  # to the left\n",
    "            observation.extend([1, 0])\n",
    "        else:\n",
    "            observation.extend([0, 1])\n",
    "\n",
    "        if self.apple_position[1] < self.snake_head[1]:  # above\n",
    "            observation.extend([1, 0])\n",
    "        else:\n",
    "            observation.extend([0, 1])\n",
    "\n",
    "        positions = self.snake_position.copy()\n",
    "        head = positions[0].copy()\n",
    "\n",
    "        # Check left\n",
    "        head_left = [head[0] - 10, head[1]]\n",
    "        positions[0] = head_left\n",
    "        o = collision_with_boundaries(head_left) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check right\n",
    "        head_right = [head[0] + 10, head[1]]\n",
    "        positions[0] = head_right\n",
    "        o = collision_with_boundaries(head_right) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check above\n",
    "        head_above = [head[0], head[1] - 10]\n",
    "        positions[0] = head_above\n",
    "        o = collision_with_boundaries(head_above) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        # Check below\n",
    "        head_below = [head[0], head[1] + 10]\n",
    "        positions[0] = head_below\n",
    "        o = collision_with_boundaries(head_below) or collision_with_self(positions)\n",
    "        observation.append(o)\n",
    "\n",
    "        observation = np.array(observation, dtype=np.float32)\n",
    "        return observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4844f7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
