{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26350157",
   "metadata": {},
   "source": [
    "# Kaj so nevronske mreže\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069fb1f9",
   "metadata": {},
   "source": [
    "# Nevron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571754be",
   "metadata": {},
   "source": [
    "Izhodna vrednost enega nevrona se izračuna po enačbi:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa7999",
   "metadata": {},
   "source": [
    "$\\Large o = \\sum_{j=0}^{n}(i_j w_j) + b$\n",
    "\n",
    "* $i_j$ je specifičen input\n",
    "* $w_j$ je weight specifičnega inputa\n",
    "* $b$   je bias nevrona"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c211ba3",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e7b672",
   "metadata": {},
   "source": [
    "![Nevron](images/01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9634c8",
   "metadata": {},
   "source": [
    "Izhodno vrednost dobimo po zgornji enačbi:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a61215",
   "metadata": {},
   "source": [
    "$o = \\sum_{j=0}^{n}(i_j w_j) + b = i_0 \\cdot w_0 + i_1 \\cdot w_1 + i_2 \\cdot w_2 + b = 1 \\cdot 0.2 + 2 \\cdot 0.8 + 3 \\cdot (-0.5) + 2 = 2.3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f65814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1,2,3]\n",
    "weights = [0.2, 0.8, -0.5]\n",
    "bias = 2\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d139e28",
   "metadata": {},
   "source": [
    "![Nevron - 4 inputs](images/02.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc07ff",
   "metadata": {},
   "source": [
    "$o = \\sum_{j=0}^{n}(i_j w_j) + b = i_0 \\cdot w_0 + i_1 \\cdot w_1 + i_2 \\cdot w_2 + i_3 \\cdot w_3 + b = 1 \\cdot 0.2 + 2 \\cdot 0.8 + 3 \\cdot (-0.5) + 2.5 \\cdot 1.0 + 2 = 4.8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b4ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1,2,3,2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c749b2d",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149a15d6",
   "metadata": {},
   "source": [
    "**Dot product**\n",
    "\n",
    "$\\Large \\vec{a}^{\\,}\\cdot \\vec{b}^{\\,} = [1,2,3]\\cdot [2,3,4] = 1\\cdot 2 + 2\\cdot 3 + 3\\cdot 4 = 20$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83b6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primer računanja s tensorji za en neuron\n",
    "import numpy as np\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2.0\n",
    "\n",
    "\n",
    "output = np.dot(weights, inputs) + bias\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb58ded",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5a3375",
   "metadata": {},
   "source": [
    "# Plast nevronov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158331cf",
   "metadata": {},
   "source": [
    "![Dense layer](images/03.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b796c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1, 2, 3, 2.5]                \n",
    "weights1 = [ 0.20,  0.80, -0.50,  1.00]  # uteži neurona1\n",
    "weights2 = [ 0.50, -0.91,  0.26, -0.50]  # uteži neurona2\n",
    "weights3 = [-0.26, -0.27,  0.17,  0.87]  # uteži neurona3\n",
    "\n",
    "bias1 = 2.0  # bias neurona1\n",
    "bias2 = 3.0  # bias neurona2\n",
    "bias3 = 0.5  # bias neurona3\n",
    "\n",
    "layer_outputs = [\n",
    "    # Output Neuron 1:\n",
    "    inputs[0]*weights1[0] +\n",
    "    inputs[1]*weights1[1] +\n",
    "    inputs[2]*weights1[2] +\n",
    "    inputs[3]*weights1[3] + bias1,\n",
    "\n",
    "    # Output Neuron 2:\n",
    "    inputs[0]*weights2[0] +\n",
    "    inputs[1]*weights2[1] +\n",
    "    inputs[2]*weights2[2] +\n",
    "    inputs[3]*weights2[3] + bias2,\n",
    "\n",
    "    # Output Neuron 3:\n",
    "    inputs[0]*weights3[0] +\n",
    "    inputs[1]*weights3[1] +\n",
    "    inputs[2]*weights3[2] +\n",
    "    inputs[3]*weights3[3] + bias3\n",
    "]\n",
    "\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff308861",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93bfba",
   "metadata": {},
   "source": [
    "Ista koda, zapisana z **numpy.dot()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6870bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights = [[ 0.20,  0.80, -0.50,  1.00],  # uteži neurona1\n",
    "           [ 0.50, -0.91,  0.26, -0.50],  # uteži neurona2\n",
    "           [-0.26, -0.27,  0.17,  0.87]]  # uteži neurona1\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "layer_outputs = np.dot(weights, inputs) + biases\n",
    "\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238d589f",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96313dc3",
   "metadata": {},
   "source": [
    "![2 Dense layers](images/04.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61306b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "\n",
    "# Dense layer 1\n",
    "weights = [[ 0.20,  0.80, -0.50,  1.00],  # uteži neurona1\n",
    "           [ 0.50, -0.91,  0.26, -0.50],  # uteži neurona2\n",
    "           [-0.26, -0.27,  0.17,  0.87]]  # uteži neurona3\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "# Dense layer 2\n",
    "weights2 = [[ 0.10, -0.14,  0.50],  # uteži neurona1\n",
    "            [-0.50,  0.12, -0.33],  # uteži neurona2\n",
    "            [-0.44,  0.73, -0.13]]  # uteži neurona3\n",
    "biases2 = [-1, 2, -0.5]\n",
    "\n",
    "\n",
    "layer1_outputs = np.dot(weights, inputs) + biases\n",
    "layer2_outputs = np.dot(weights2, layer1_outputs) + biases2\n",
    "\n",
    "print(\"Layer 1: \", layer1_outputs)\n",
    "print(\"Layer 2: \", layer2_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14c8aca",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96d1a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "\n",
    "\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_neurons, n_inputs)\n",
    "        print(\"Weights: \")\n",
    "        print(self.weights)\n",
    "        self.biases = np.zeros(n_neurons)\n",
    "        print(\"Bias: \")\n",
    "        print(self.biases)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(self.weights, inputs) + self.biases\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "\n",
    "print(\"Creating DENSE 1\")\n",
    "dense1 = Layer_Dense(4, 3)\n",
    "print(30*\"=\")\n",
    "\n",
    "print(\"Creating DENSE 2\")\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "print(30*\"=\")\n",
    "\n",
    "dense1.forward(inputs)\n",
    "dense2.forward(dense1.output)\n",
    "\n",
    "print(\"Dense 1 output: \")\n",
    "print(dense1.output)\n",
    "print()\n",
    "\n",
    "print(\"Dense 2 outut: \")\n",
    "print(dense2.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85c7bd1",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1710517",
   "metadata": {},
   "source": [
    "Za primer vzemimo batch, kjer imamo 3 vzorce:\n",
    "```python\n",
    "inputs = [[1.0, 2.0, 3.0, 2.5],  # sample 1\n",
    "          [2.0, 5.0, -1.0, 2.0], # sample 2\n",
    "          [-1.5, 2.7, 3.3, -0.8]]# sample 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ce6d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [[ 1.0, 2.0,  3.0,  2.5],  # first  inputs to layer1\n",
    "          [ 2.0, 5.0, -1.0,  2.0],  # second inputs to layer1\n",
    "          [-1.5, 2.7,  3.3, -0.8]]  # third  inputs to layer1\n",
    "\n",
    "weights = [[ 0.20,  0.80, -0.50,  1.00],  # uteži neurona1\n",
    "           [ 0.50, -0.91,  0.26, -0.50],  # uteži neurona2\n",
    "           [-0.26, -0.27,  0.17,  0.87]]  # uteži neurona3\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "\n",
    "layer_outputs = np.dot(np.array(inputs), np.array(weights)) + biases\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ff204",
   "metadata": {},
   "source": [
    "![Matrix calculation](images/05.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "inputs = [[ 1.0, 2.0,  3.0,  2.5],  # first  inputs to layer1\n",
    "          [ 2.0, 5.0, -1.0,  2.0],  # second inputs to layer1\n",
    "          [-1.5, 2.7,  3.3, -0.8]]  # third  inputs to layer1\n",
    "\n",
    "#          # N1    # N2    # N3\n",
    "weights = [[ 0.2 ,  0.5 , -0.26],\n",
    "           [ 0.8 , -0.91, -0.27],\n",
    "           [-0.5 ,  0.26,  0.17],\n",
    "           [ 1.  , -0.5 ,  0.87]]\n",
    "\n",
    "#        # N1   # N2  # N3\n",
    "biases = [2.0,   3.0,  0.5]\n",
    "\n",
    "layer_outputs = np.dot(np.array(inputs), np.array(weights)) + biases\n",
    "\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbddc71",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a87fd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        \n",
    "        # <=== HERE ===>\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        # <=== HERE ===>\n",
    "        \n",
    "        print(\"Weights: \")\n",
    "        print(self.weights)\n",
    "        \n",
    "        # <=== HERE ===>\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # <=== HERE ===>\n",
    "        \n",
    "        print(\"Bias: \")\n",
    "        print(self.biases)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        \n",
    "        # <=== HERE ===>\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        # <=== HERE ===>\n",
    "        \n",
    "inputs = np.array([[ 1.0, 2.0,  3.0,  2.5],   # first  inputs to layer1\n",
    "                   [ 2.0, 5.0, -1.0,  2.0],   # second inputs to layer1\n",
    "                   [-1.5, 2.7,  3.3, -0.8]])  # third  inputs to layer1\n",
    "print(\"Inputs:\")\n",
    "print(inputs)\n",
    "\n",
    "dense1 = Layer_Dense(4,3)\n",
    "dense1.forward(inputs)\n",
    "\n",
    "print(\"Dense OUTPUT\")\n",
    "print(dense1.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31da608",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36606ca0",
   "metadata": {},
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c10197",
   "metadata": {},
   "source": [
    "![Activation function](images/06.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd73339",
   "metadata": {},
   "source": [
    "## Step Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c211c162",
   "metadata": {},
   "source": [
    "$\\Large \n",
    "y = \n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t1  & x > 0 \\\\\n",
    "        0  & x \\leq 0\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88d3a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def step_function(x):\n",
    "    return 1 if x>0 else 0\n",
    "\n",
    "X = np.arange(-10, 10, 0.01)\n",
    "y = [step_function(x) for x in X]\n",
    "\n",
    "plt.plot(X, y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70ef9a",
   "metadata": {},
   "source": [
    "## Sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040d5056",
   "metadata": {},
   "source": [
    "$\\Large y = \\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf276ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / ( 1 + np.exp(-x))\n",
    "\n",
    "X = np.arange(-10, 10, 0.01)\n",
    "y = [sigmoid(x) for x in X]\n",
    "\n",
    "plt.plot(X, y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d616143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanishing gradient problem\n",
    "print(f\"{sigmoid(20):.50f}\")\n",
    "print(f\"{sigmoid(21):.50f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba9327f",
   "metadata": {},
   "source": [
    "## Rectified Linear Units - ReLU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b44cba",
   "metadata": {},
   "source": [
    "$\\Large \n",
    "y = \n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\tx  & x > 0 \\\\\n",
    "        0  & x \\leq 0\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897572f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return x if x>0 else 0\n",
    "\n",
    "X = np.arange(-10, 10, 0.01)\n",
    "y = [relu(x) for x in X]\n",
    "\n",
    "plt.plot(X, y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06976ed0",
   "metadata": {},
   "source": [
    "## Uporaba aktivacijske funkcije v naši neuronski merži"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c74e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "\n",
    "class Activation_ReLU:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output valzes from input\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "n_samples = 4\n",
    "neurons = 3\n",
    "X = np.random.normal(size=(n_samples,neurons))\n",
    "print(\"Inputs:\")\n",
    "print(X)\n",
    "\n",
    "activation = Activation_ReLU()\n",
    "activation.forward(X)\n",
    "print(\"Output\")\n",
    "print(activation.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67160403",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce94475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        print(\"Weights: \")\n",
    "        print(self.weights)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        print(\"Bias: \")\n",
    "        print(self.biases)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# <=== HERE ===>\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output valzes from input\n",
    "        self.output = np.maximum(0, inputs)\n",
    "# <=== HERE ===>\n",
    "        \n",
    "        \n",
    "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "print(\"Ustvarimo DENSE 1\")\n",
    "dense1 = Layer_Dense(4, 3)\n",
    "# <=== HERE ===>\n",
    "activation1 = Activation_ReLU()\n",
    "# <=== HERE ===>\n",
    "print(\"Ustvarimo DENSE 2\")\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "dense1.forward(inputs)\n",
    "# <=== HERE ===>\n",
    "activation1.forward(dense1.output)\n",
    "# <=== HERE ===>\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "print(30*\"=\")\n",
    "print()\n",
    "\n",
    "print(\"Dense 1 output: \")\n",
    "print(dense1.output)\n",
    "# <=== HERE ===>\n",
    "print(\"ReLU 1 output:\")\n",
    "print(activation1.output)\n",
    "# <=== HERE ===>\n",
    "print(\"Dense 2 output: \")\n",
    "print(dense2.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf7e2d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a8f949",
   "metadata": {},
   "source": [
    "## Classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def vertical_data(samples, classes):\n",
    "    X = np.zeros((samples*classes, 2))\n",
    "    y = np.zeros(samples*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(samples*class_number, samples*(class_number+1))\n",
    "        X[ix] = np.c_[np.random.randn(samples)*.1 + (class_number)/3, np.random.randn(samples)*.1 + 0.5]\n",
    "        y[ix] = class_number\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a189590",
   "metadata": {},
   "source": [
    "## Softmax Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f460b5e",
   "metadata": {},
   "source": [
    "$\\Large S_j = \\frac{e^{o_j}}{\\sum_{l=0}^{L}e^{o_l}}$\n",
    "\n",
    "* $S_j$ je confidence score $j$ razreda\n",
    "* $o_j$ je izhodna vrednost neurona\n",
    "* $\\sum_{l=0}^{L}e^{o_l}$ je seštevek $e^o$ vseh izhodnih vrednosti neuronov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c3837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(inputs):\n",
    "    exp_values = np.exp(inputs)\n",
    "    return exp_values / np.sum(exp_values)\n",
    "\n",
    "layer_outputs = [1.0, 2.0, 3.0, 2.5]\n",
    "\n",
    "softmax_output = softmax(layer_outputs)\n",
    "print(softmax_output)\n",
    "print(sum(softmax_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d35adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs)\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "        \n",
    "layer_outputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "softmax = Activation_Softmax()\n",
    "softmax.forward(layer_outputs)\n",
    "print(softmax.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a0c596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c087cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(inputs):\n",
    "    exp_values = np.exp(inputs)\n",
    "    return exp_values / np.sum(exp_values)\n",
    "\n",
    "layer_outputs = np.array([1_000, 2_000, 3_000, 2_500])\n",
    "print(\"Layer outputs:\")\n",
    "print(layer_outputs)\n",
    "\n",
    "softmax_output = softmax(layer_outputs)\n",
    "print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd38b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb10109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(inputs):\n",
    "    exp_values = np.exp(inputs)\n",
    "    return exp_values / np.sum(exp_values)\n",
    "\n",
    "# ============================================\n",
    "layer_outputs = np.array([1, 2, 3, 2.5])\n",
    "print(\"Layer outputs:\")\n",
    "print(layer_outputs)\n",
    "\n",
    "softmax_output = softmax(layer_outputs)\n",
    "print(softmax_output)\n",
    "\n",
    "print(50*\"*\")\n",
    "# ============================================\n",
    "\n",
    "layer_outputs = np.array([1, 2, 3, 2.5])\n",
    "\n",
    "layer_outputs = layer_outputs - layer_outputs.max()\n",
    "print(\"Layer outputs:\")\n",
    "print(layer_outputs)\n",
    "\n",
    "softmax_output = softmax(layer_outputs)\n",
    "print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37510fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e444eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "        \n",
    "layer_outputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "softmax = Activation_Softmax()\n",
    "softmax.forward(layer_outputs)\n",
    "print(softmax.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d613df",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c74a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "# <=== HERE ===>      \n",
    "        \n",
    "# <=== HERE ===>\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "# <=== HERE ===>\n",
    "\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "# <=== HERE ===>\n",
    "activation2 = Activation_Softmax()\n",
    "# <=== HERE ===>\n",
    "\n",
    "# <=== HERE ===>\n",
    "dense1.forward(X[:5])\n",
    "# <=== HERE ===>\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "# <=== HERE ===>\n",
    "activation2.forward(dense2.output)\n",
    "# <=== HERE ===>\n",
    "\n",
    "print(\"Dense 1 output: \")\n",
    "print(dense1.output)\n",
    "print(\"ReLU 1 output:\")\n",
    "print(activation1.output)\n",
    "print(\"Dense 2 outut: \")\n",
    "print(dense2.output)\n",
    "# <=== HERE ===>\n",
    "print(50*\"*\")\n",
    "print(\"Softmax output - PREDICTION\")\n",
    "print(activation2.output)\n",
    "# <=== HERE ===>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325f537",
   "metadata": {},
   "source": [
    "# Loss Function and Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991036bb",
   "metadata": {},
   "source": [
    "## Categorical Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f442dc1",
   "metadata": {},
   "source": [
    "$\\Large L = - \\sum_{j}y_j log(\\hat{y_j})$\n",
    "\n",
    "* $L$ je **loss** vrednost\n",
    "* $y_j$ je resnična vrednost\n",
    "* $\\hat{y_j}$ je napovedana vrednost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b753f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [0.7, 0.1, 0.2]\n",
    "real_values =  [1, 0, 0]\n",
    "\n",
    "loss = -(real_values[0]*np.log(inputs[0]) +\n",
    "         real_values[1]*np.log(inputs[1]) +\n",
    "         real_values[2]*np.log(inputs[2]))\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8546c3ff",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34efb34a",
   "metadata": {},
   "source": [
    "$\\Large L = - \\sum_{j}y_j log(\\hat{y_j}) = \\\\ \n",
    "\\Large -( y_0 log(\\hat{y_0}) + y_1 log(\\hat{y_1}) + y_2 log(\\hat{y_2})) = \\\\ \n",
    "\\Large -(1 \\cdot log(\\hat{y_0}) + 0 \\cdot log(\\hat{y_1}) + 0 \\cdot log(\\hat{y_2})) = \\\\\n",
    "\\Large - log(\\hat{y_0}) = - log(\\hat{y_k})$\n",
    "\n",
    "* $L$ je **loss** vrednost\n",
    "* $y_j$ je resnična vrednost\n",
    "* $\\hat{y_j}$ je napovedana vrednost\n",
    "* $k$ - index pravilnega razreda\n",
    "\n",
    "Na kratko zapisano:\n",
    "\n",
    "$\\Large L = - log(\\hat{y_k})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d453be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [0.7, 0.1, 0.2]\n",
    "real_values =  [1, 0, 0]\n",
    "\n",
    "correct_class_index = real_values.index(1)\n",
    "print(\"Correct class index: \", correct_class_index)\n",
    "\n",
    "loss = -np.log(inputs[correct_class_index])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46208345",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [0.7, 0.0, 0.3]\n",
    "real_values =  [0, 1, 0]\n",
    "\n",
    "correct_class_index = real_values.index(1)\n",
    "print(\"Correct class index: \", correct_class_index)\n",
    "loss = -np.log(inputs[correct_class_index])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54976170",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-np.log(0))        # ne-korigiran rezultat\n",
    "print(-np.log(0 + 1e-7)) # korigiran rezultat povečan za minimalno vrednost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d86d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-np.log(0.1))        # ne-korigiran rezultat\n",
    "print(-np.log(0.1 + 1e-7)) # korigiran rezultat povečan za minimalno vrednost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7823e646",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412895db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-np.log(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cf75f3",
   "metadata": {},
   "source": [
    "Da se rešimo tega problema bomo največjo številko pomanjšal za neko minimalno vrednost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-np.log(1)) # pravilna in željena vrednost\n",
    "print(-np.log(1-1e-7)) # korigirana vrednost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b9a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-np.log(0.9)) # pravilna in željena vrednost\n",
    "print(-np.log(0.9 - 1e-7)) # korigirana vrednost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39baa724",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1.7, 0.0, 0.3]\n",
    "print(\"Inputs:\")\n",
    "print(inputs)\n",
    "\n",
    "cor_inputs = np.clip(inputs, 1e-7, 1-1e-7)\n",
    "print(\"Corrected inputs:\")\n",
    "print(cor_inputs)\n",
    "\n",
    "real_values =  [0, 1, 0]\n",
    "\n",
    "correct_class_index = real_values.index(1)\n",
    "print(\"Correct class index: \", correct_class_index)\n",
    "loss = -np.log(cor_inputs[correct_class_index])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb508b1",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d38a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bda6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "        print(\"Correct confidencesa:\")\n",
    "        print(correct_confidences)\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "#class_targets = np.array([0, 2, 2])\n",
    "class_targets = np.array([[1, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 1, 0]])\n",
    "\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(softmax_outputs, class_targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3cbbd2",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67516d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "# <=== HERE ===>\n",
    "\n",
    "        \n",
    "\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "# <=== HERE ===>\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "# <=== HERE ===>\n",
    "\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Let's see output of the first few samples:\n",
    "print(\"Predictions\")\n",
    "print(activation2.output[:5])\n",
    "\n",
    "# Perform a forward pass through loss function\n",
    "# it takes the output of second dense layer here and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "# Print loss value\n",
    "print('loss:', loss)\n",
    "# <=== HERE ===>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd5207",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc5dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])\n",
    "\n",
    "# Perform a forward pass through loss function\n",
    "# it takes the output of second dense layer here and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "# Print loss value\n",
    "print('loss:', loss)\n",
    "\n",
    "\n",
    "# <=== HERE ===>\n",
    "\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"o\", s=100, edgecolors=\"black\")\n",
    "plt.show()\n",
    "# <=== HERE ===>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27c218a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea12f778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
