{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7577d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def spiral_data(samples, classes):\n",
    "    X = np.zeros((samples*classes, 2))\n",
    "    y = np.zeros(samples*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(samples*class_number, samples*(class_number+1))\n",
    "        r = np.linspace(0.0, 1, samples)\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, samples) + np.random.randn(samples)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y\n",
    "\n",
    "X, y = spiral_data(100, 3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a6f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49ee1748",
   "metadata": {},
   "source": [
    "# Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a1814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def our_function(x):\n",
    "    y = (x**2 + 2*x) + np.sin(x) + 5*np.sin(x*2) - 10*np.sin(x/3) + 5\n",
    "    return y\n",
    "\n",
    "def d_f(x):\n",
    "    df = 2*x + 2 + np.cos(x) + 10*np.cos(x*2) - (10/3) * np.cos(x/3) \n",
    "    return df\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = our_function(x)\n",
    "df = d_f(x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.plot(x, y)\n",
    "#ax.plot(x, df)\n",
    "#ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec5f465",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def our_function(x):\n",
    "    y = (x**2 + 2*x) + np.sin(x) + 5*np.sin(x*2) - 10*np.sin(x/3) + 5\n",
    "    return y\n",
    "\n",
    "def d_f(x):\n",
    "    df = 2*x + 2 + np.cos(x) + 10*np.cos(x*2) - (10/3) * np.cos(x/3) \n",
    "    return df\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = our_function(x)\n",
    "df = d_f(x)\n",
    "\n",
    "x1 = -4\n",
    "y1 = our_function(x1)\n",
    "print(f\"X1: {x1:.2f} \\t Y1: {y1:.2f}\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "dx1 = d_f(x1)\n",
    "new_x1 = x1 - learning_rate*dx1\n",
    "new_y1 = our_function(new_x1)\n",
    "print(f\"X1: {new_x1:.2f} \\t Y1: {new_y1:.2f}\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(12, 24))\n",
    "axs[0].plot(x, y, alpha=0.4)\n",
    "axs[0].scatter([x1, new_x1], [y1, new_y1], color=\"orange\", s=50)\n",
    "\n",
    "axs[1].plot([1,2], [y1, new_y1])\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c7b9ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_learning(x_path, axs):\n",
    "    axs[0].plot(x, y, linewidth=3.5, alpha=0.4) # plot the \"loss\" function\n",
    "\n",
    "    alpha_points = np.linspace(0.4, 1, len(x_path))\n",
    "    for i in range(1, len(alpha_points)):\n",
    "        xs = np.array([ x_path[i-1], x_path[i] ]) # get two by two points and plot every line with different alpha\n",
    "        axs[0].plot(xs, our_function(xs), c=\"orange\", marker=\"o\", alpha=alpha_points[i])\n",
    "    \n",
    "    loss_values = our_function(x_path)\n",
    "    axs[1].plot(range(len(x_path)), loss_values)\n",
    "    axs[1].set_ylabel(\"Loss\")\n",
    "        \n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = our_function(x)\n",
    "\n",
    "x_path = [-4.5]\n",
    "#x_path = [np.random.randint(-5, 5)]\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "for i in range(epochs):\n",
    "    current_x = x_path[i]\n",
    "    d_value = d_f(current_x)\n",
    "    new_x = current_x - learning_rate*d_value\n",
    "    x_path.append(new_x)\n",
    "\n",
    "x_path = np.array(x_path)\n",
    "for x_ in x_path:\n",
    "    print(f\"Loss: {our_function(x_)}\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(12, 24))\n",
    "plot_learning(x_path, axs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244a1f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_learning(x_path, axs, show_n_xpath=10):\n",
    "    axs[0].plot(x, y, linewidth=3.5, alpha=0.4) # plot the \"loss\" function\n",
    "\n",
    "    alpha_points = np.linspace(0.4, 1, len(x_path[:show_n_xpath]))\n",
    "    for i in range(1, len(alpha_points)):\n",
    "        xs = np.array([ x_path[i-1], x_path[i] ]) # get two by two points and plot every line with different alpha\n",
    "        axs[0].plot(xs, our_function(xs), c=\"orange\", marker=\"o\", alpha=alpha_points[i])\n",
    "    \n",
    "    loss_values = our_function(x_path)\n",
    "    axs[1].plot(range(len(x_path)), loss_values)\n",
    "    axs[1].set_ylabel(\"Loss\")\n",
    "        \n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = our_function(x)\n",
    "\n",
    "x_path = [-4.5]\n",
    "#x_path = [np.random.randint(-5, 5)]\n",
    "# <=== HERE ===>\n",
    "epochs = 25\n",
    "learning_rate = 1\n",
    "# <=== HERE ===>\n",
    "for i in range(epochs):\n",
    "    current_x = x_path[i]\n",
    "    d_value = d_f(current_x)\n",
    "    new_x = current_x - learning_rate*d_value\n",
    "    x_path.append(new_x)\n",
    "\n",
    "x_path = np.array(x_path)\n",
    "for x_ in x_path:\n",
    "    print(f\"Loss: {our_function(x_)}\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(12, 24))\n",
    "plot_learning(x_path, axs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a34ed9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_learning(x_path, axs, show_n_xpath=10):\n",
    "    axs[0].plot(x, y, linewidth=3.5, alpha=0.4) # plot the \"loss\" function\n",
    "\n",
    "    alpha_points = np.linspace(0.4, 1, len(x_path[:show_n_xpath]))\n",
    "    for i in range(1, len(alpha_points)):\n",
    "        xs = np.array([ x_path[i-1], x_path[i] ]) # get two by two points and plot every line with different alpha\n",
    "        axs[0].plot(xs, our_function(xs), c=\"orange\", marker=\"o\", alpha=alpha_points[i])\n",
    "    \n",
    "    loss_values = our_function(x_path)\n",
    "    axs[1].plot(range(len(x_path)), loss_values)\n",
    "    axs[1].set_ylabel(\"Loss\")\n",
    "        \n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = our_function(x)\n",
    "\n",
    "x_path = [-4.5]\n",
    "#x_path = [np.random.randint(-5, 5)]\n",
    "# <=== HERE ===>\n",
    "epochs = 10\n",
    "learning_rate = 0.19\n",
    "# <=== HERE ===>\n",
    "for i in range(epochs):\n",
    "    current_x = x_path[i]\n",
    "    d_value = d_f(current_x)\n",
    "    new_x = current_x - learning_rate*d_value\n",
    "    x_path.append(new_x)\n",
    "\n",
    "x_path = np.array(x_path)\n",
    "for x_ in x_path:\n",
    "    print(f\"Loss: {our_function(x_)}\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(12, 24))\n",
    "plot_learning(x_path, axs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c1eaed",
   "metadata": {},
   "source": [
    "# Learning Rate Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361a0ea3",
   "metadata": {},
   "source": [
    "Splošna enačba za learning decay je:\n",
    "\n",
    "$\\Large l_r = l_{r start} * \\frac{1}{1 + decay * step} $\n",
    "\n",
    "* $l_r$ - new learning rate\n",
    "* $l_{r start}$ - starting learning rate\n",
    "* $desay$ - learning rate decay\n",
    "* $step$ - current step we are on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff94d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lr(step):\n",
    "    return starting_learning_rate * (1. / (1 + learning_rate_decay * step))\n",
    "\n",
    "starting_learning_rate = 1.\n",
    "learning_rate_decay = 0.1\n",
    "steps = np.linspace(0,100,100)\n",
    "\n",
    "learning_rate = calc_lr(steps)\n",
    "\n",
    "\n",
    "plt.plot(steps, learning_rate)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda7398b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# <=== HERE ===>\n",
    "def calc_lr(step):\n",
    "    return starting_learning_rate * (1. / (1 + learning_rate_decay * step))\n",
    "\n",
    "starting_learning_rate = 0.19\n",
    "learning_rate_decay = 0.1\n",
    "# <=== HERE ===>\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = our_function(x)\n",
    "\n",
    "x_path = [-4.5]\n",
    "epochs = 10\n",
    "\n",
    "for i in range(epochs):\n",
    "    current_x = x_path[i]\n",
    "    d_value = d_f(current_x)\n",
    "    # <=== HERE ===>\n",
    "    new_x = current_x - calc_lr(i)*d_value\n",
    "    # <=== HERE ===>\n",
    "    x_path.append(new_x)\n",
    "\n",
    "x_path = np.array(x_path)\n",
    "for x_ in x_path:\n",
    "    print(f\"Loss: {our_function(x_)}\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(12, 24))\n",
    "plot_learning(x_path, axs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f5c43d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calc_lr(step):\n",
    "    return starting_learning_rate * (1. / (1 + learning_rate_decay * step))\n",
    "\n",
    "starting_learning_rate = 0.19\n",
    "learning_rate_decay = 0.1\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = our_function(x)\n",
    "\n",
    "x_path = [-4.5]\n",
    "# <=== HERE ===>\n",
    "epochs = 50\n",
    "# <=== HERE ===>\n",
    "\n",
    "for i in range(epochs):\n",
    "    current_x = x_path[i]\n",
    "    d_value = d_f(current_x)\n",
    "    new_x = current_x - calc_lr(i)*d_value\n",
    "    x_path.append(new_x)\n",
    "\n",
    "x_path = np.array(x_path)\n",
    "for x_ in x_path:\n",
    "    print(f\"Loss: {our_function(x_)}\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(12, 24))\n",
    "plot_learning(x_path, axs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f65727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b9caf9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2020)\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "        \n",
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 128)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(128, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(learning_rate=1, decay=1e-4)\n",
    "\n",
    "# Train in loop\n",
    "losses = [] # Used to plot loss values and see how our model learned\n",
    "for epoch in range(10_001):\n",
    "\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        losses.append(loss)\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "    \n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(12, 24))\n",
    "axs[0].scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "ax2 = axs[0].twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"o\", s=100, edgecolors=\"black\")\n",
    "\n",
    "axs[1].plot(range(len(losses)), losses)\n",
    "axs[1].set_xlabel(\"100 * Epoch\")\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ad37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "\n",
    "# Create test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X_test[:, 0], X_test[:, 1], c=predictions, cmap=\"brg\", marker=\"o\", s=100, edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e75c35",
   "metadata": {},
   "source": [
    "![Loss krivulje](images/loss_krivulje.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a769b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3deab5",
   "metadata": {},
   "source": [
    "# L1 and L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ae1cd7",
   "metadata": {},
   "source": [
    "**L2 Regularization penalty for weights**\n",
    "\n",
    "$\\Large L_{2w} = \\lambda \\sum_m w_m^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef2fa1",
   "metadata": {},
   "source": [
    "**L2 Regularization penalty for biases**\n",
    "\n",
    "$\\Large L_{2b} = \\lambda \\sum_n b_n^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc98ab8",
   "metadata": {},
   "source": [
    "**L1 Regularization penalty for weights**\n",
    "\n",
    "$\\Large L_{1w} = \\lambda \\sum_m |w_m| $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc8716",
   "metadata": {},
   "source": [
    "**L1 Regularization penalty for biases**\n",
    "\n",
    "$\\Large L_{1b} = \\lambda \\sum_n |b_n| $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fd9991",
   "metadata": {},
   "source": [
    "$\\Large total\\_loss = loss + L_{1w} + L_{1w} + L_{2w} + L_{2b} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer initialization\n",
    "def __init__(self, n_inputs, n_neurons, \n",
    "             weight_regularizer_l1=0, \n",
    "             weight_regularizer_l2=0,\n",
    "             bias_regularizer_l1=0, \n",
    "             bias_regularizer_l2=0):\n",
    "    # Initialize weights and biases\n",
    "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "    self.biases = np.zeros((1, n_neurons))\n",
    "    # Set regularization strength\n",
    "    self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "    self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "    self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "    self.bias_regularizer_l2 = bias_regularizer_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a02664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization loss calculation\n",
    "def regularization_loss(self, layer):\n",
    "\n",
    "    # 0 by default\n",
    "    regularization_loss = 0\n",
    "\n",
    "    # L1 regularization - weights\n",
    "    # calculate only when factor greater than 0\n",
    "    if layer.weight_regularizer_l1 > 0:\n",
    "        regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "    # L2 regularization - weights\n",
    "    if layer.weight_regularizer_l2 > 0:\n",
    "        regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "    # L1 regularization - biases\n",
    "    # calculate only when factor greater than 0\n",
    "    if layer.bias_regularizer_l1 > 0:\n",
    "        regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "\n",
    "    # L2 regularization - biases\n",
    "    if layer.bias_regularizer_l2 > 0:\n",
    "        regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "    return regularization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf8ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss from output of activation2 so softmax activation\n",
    "data_loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "# Calculate regularization penalty\n",
    "regularization_loss = loss_function.regularization_loss(dense1) + \\\n",
    "                      loss_function.regularization_loss(dense2)\n",
    "\n",
    "# Calculate overall loss\n",
    "loss = data_loss + regularization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cd7c26",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d28912",
   "metadata": {},
   "source": [
    "**L2 odvod uteži**\n",
    "\n",
    "$\\Large \\frac{\\partial L_{2w}}{\\partial w_m} = \\frac{\\partial }{\\partial w_m}( \\lambda \\sum_{m}w_m^2) = \\lambda \\frac{\\partial }{\\partial w_m}w_m^2 = \\lambda 2 w_m$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6280cbe",
   "metadata": {},
   "source": [
    "**L2 odvod bias**\n",
    "\n",
    "$\\Large \\frac{\\partial L_{2b}}{\\partial b_n} = \\frac{\\partial }{\\partial b_n}( \\lambda \\sum_{n}b_n^2) = \\lambda \\frac{\\partial }{\\partial b_n}b_n^2 = \\lambda 2 b_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb3fad",
   "metadata": {},
   "source": [
    "$\\Large \n",
    "|x| = \n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\tx  & x > 0 \\\\\n",
    "        -x  & x < 0\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$\n",
    "\n",
    "Odvod absolutne funkcije je:\n",
    "\n",
    "$\\Large \n",
    "|x|' = \n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t1  & x > 0 \\\\\n",
    "        -1  & x < 0\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddb79f1",
   "metadata": {},
   "source": [
    "**L1 odvod uteži**\n",
    "\n",
    "$\\Large \\frac{\\partial L_{1w}}{\\partial w_m} = \\frac{\\partial }{\\partial w_m}( \\lambda \\sum_m {|w_m|}) = \\lambda \\frac{\\partial }{\\partial w_m} |w_m| = \\lambda \\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t1  & w_m > 0 \\\\\n",
    "        -1  & w_m < 0\n",
    "\t\\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebabc6f",
   "metadata": {},
   "source": [
    "**L1 odvod bias**\n",
    "\n",
    "$\\Large \\frac{\\partial L_{1w}}{\\partial b_n} = \\frac{\\partial }{\\partial b_n}( \\lambda \\sum_n {|b_n|}) = \\lambda \\frac{\\partial }{\\partial b_n} |b_n| = \\lambda \\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t1  & b_n > 0 \\\\\n",
    "        -1  & b_n < 0\n",
    "\t\\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e750ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.2, 0.8, -0.5]\n",
    "weight_regularizer_l1 = 0.1 # lambda\n",
    "dL1 = []  # array of partial derivatives of L1 regularization\n",
    "for weight in weights:\n",
    "    if weight >= 0:\n",
    "        dL1.append(weight_regularizer_l1*1)\n",
    "    else:\n",
    "        dL1.append(weight_regularizer_l1*-1)\n",
    "print(dL1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae6d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [[0.2, 0.8, -0.5, 1],  # now we have 3 sets of weights\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "weight_regularizer_l1 = 0.1 # lambda\n",
    "dL1 = []  # array of partial derivatives of L1 regularization\n",
    "for neuron in weights:\n",
    "    neuron_dL1 = []  # derivatives related to one neuron\n",
    "    for weight in neuron:\n",
    "        if weight >= 0:\n",
    "            neuron_dL1.append(weight_regularizer_l1*1)\n",
    "        else:\n",
    "            neuron_dL1.append(weight_regularizer_l1*-1)\n",
    "    dL1.append(neuron_dL1)\n",
    "\n",
    "for neuron in dL1:\n",
    "    print(neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d091f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]])\n",
    "weight_regularizer_l1 = 0.1 # lambda\n",
    "dL1 = np.ones_like(weights)\n",
    "dL1[weights < 0] = -1\n",
    "dL1 = dL1 * weight_regularizer_l1\n",
    "\n",
    "print(dL1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de12fd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10e9d25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2020)\n",
    "\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                         weight_regularizer_l1=0, \n",
    "                         weight_regularizer_l2=0,\n",
    "                         bias_regularizer_l1=0, \n",
    "                         bias_regularizer_l2=0):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "    \n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self, layer):\n",
    "\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "\n",
    "        # L1 regularization - weights\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "\n",
    "        # L1 regularization - biases\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "\n",
    "        # L2 regularization - biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "        \n",
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 128)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(128, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(learning_rate=1, decay=1e-4)\n",
    "\n",
    "# Train in loop\n",
    "losses = [] # Used to plot loss values and see how our model learned\n",
    "for epoch in range(10_001):\n",
    "\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = \\\n",
    "        loss_activation.loss.regularization_loss(dense1) + \\\n",
    "        loss_activation.loss.regularization_loss(dense2)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        losses.append(data_loss)\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "    \n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(12, 24))\n",
    "axs[0].scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "ax2 = axs[0].twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"o\", s=100, edgecolors=\"black\")\n",
    "\n",
    "axs[1].plot(range(len(losses)), losses)\n",
    "axs[1].set_xlabel(\"100 * Epoch\")\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bd68a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "\n",
    "# Create test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'acc: {accuracy:.3f}, loss: {loss:.3f}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X_test[:, 0], X_test[:, 1], c=predictions, cmap=\"brg\", marker=\"o\", s=100, edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc226d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123f3f69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2020)\n",
    "\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                         weight_regularizer_l1=0, \n",
    "                         weight_regularizer_l2=0,\n",
    "                         bias_regularizer_l1=0, \n",
    "                         bias_regularizer_l2=0):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "    \n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self, layer):\n",
    "\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "\n",
    "        # L1 regularization - weights\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "\n",
    "        # L1 regularization - biases\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "\n",
    "        # L2 regularization - biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "        \n",
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=500, classes=3)\n",
    "# <=== HERE ===>\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 128)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(128, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(learning_rate=1, decay=1e-4)\n",
    "\n",
    "# Train in loop\n",
    "losses = [] # Used to plot loss values and see how our model learned\n",
    "for epoch in range(10_001):\n",
    "\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = \\\n",
    "        loss_activation.loss.regularization_loss(dense1) + \\\n",
    "        loss_activation.loss.regularization_loss(dense2)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        losses.append(loss)\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "    \n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(12, 24))\n",
    "axs[0].scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "ax2 = axs[0].twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"o\", s=100, edgecolors=\"black\")\n",
    "\n",
    "axs[1].plot(range(len(losses)), losses)\n",
    "axs[1].set_xlabel(\"100 * Epoch\")\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4053e487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "\n",
    "# Create test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'acc: {accuracy:.3f}, loss: {loss:.3f}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X_test[:, 0], X_test[:, 1], c=predictions, cmap=\"brg\", marker=\"o\", s=100, edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cea516b",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
